{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10202111,"sourceType":"datasetVersion","datasetId":6304573},{"sourceId":10650878,"sourceType":"datasetVersion","datasetId":6595189}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Story Generation1","metadata":{"execution":{"iopub.status.busy":"2024-12-15T06:49:21.688460Z","iopub.execute_input":"2024-12-15T06:49:21.689375Z","iopub.status.idle":"2024-12-15T06:49:22.821222Z","shell.execute_reply.started":"2024-12-15T06:49:21.689335Z","shell.execute_reply":"2024-12-15T06:49:22.820144Z"}}},{"cell_type":"markdown","source":"## Remove Temporary Directory","metadata":{}},{"cell_type":"code","source":"!rm -r '/kaggle/working/wandb'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T05:56:57.658119Z","iopub.execute_input":"2025-02-04T05:56:57.658913Z","iopub.status.idle":"2025-02-04T05:56:58.729620Z","shell.execute_reply.started":"2025-02-04T05:56:57.658876Z","shell.execute_reply":"2025-02-04T05:56:58.728638Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"## Extract 50k dataset because of resource","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T05:57:20.329058Z","iopub.execute_input":"2025-02-04T05:57:20.329561Z","iopub.status.idle":"2025-02-04T05:57:20.334892Z","shell.execute_reply.started":"2025-02-04T05:57:20.329523Z","shell.execute_reply":"2025-02-04T05:57:20.333958Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import random\n\ndef generate_random_pairs(input_file, output_file, num_pairs=50000):\n    # Read the dataset from the input file\n    with open(input_file, 'r') as file:\n        data = file.read()\n\n    # Split the data into individual pairs (Keywords and Story)\n    pairs = data.split('<|startoftext|>')\n\n    # Filter out empty strings or any non-valid pair\n    pairs = [pair.strip() for pair in pairs if pair.strip()]\n\n    # Select a random subset of pairs (ensure we don't exceed the available number)\n    random_pairs = random.sample(pairs, min(num_pairs, len(pairs)))\n\n    # Rebuild the text in the same format\n    output_text = \"\"\n    for pair in random_pairs:\n        output_text += f\"<|startoftext|>{pair}\\n\"\n\n    # Write the result to the output file\n    with open(output_file, 'w') as file:\n        file.write(output_text)\n\n# Example usage:\ninput_file = '/kaggle/input/keyword-story-dataset/keyword_story_dataset.txt'  # Replace with the path to your input file\noutput_file = '/kaggle/working/keyword_story_dataset_50k.txt'  # Replace with the path where you want to save the output\n\ngenerate_random_pairs(input_file, output_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T05:57:23.615792Z","iopub.execute_input":"2025-02-04T05:57:23.616502Z","iopub.status.idle":"2025-02-04T05:57:57.957665Z","shell.execute_reply.started":"2025-02-04T05:57:23.616465Z","shell.execute_reply":"2025-02-04T05:57:57.956922Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset, Dataset\nimport torch\n\n# 1. Load and Preprocess Dataset\ndef load_and_preprocess_data(file_path):\n    # Read the dataset file\n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = f.read().split('<|endoftext|>')  # Split stories\n    \n    # Prepare dataset format\n    examples = [{\"text\": text.strip() + \"<|endoftext|>\"} for text in data if text.strip()]\n    return Dataset.from_list(examples)\n\n# Tokenizer function: Include 'labels'\ndef tokenize_function(examples, tokenizer, max_length=512):\n    tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Copy input_ids to labels for loss calculation\n    return tokenized\n\n# 2. Load GPT-2 Model and Tokenizer\nmodel_name = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Add special tokens\ntokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\nmodel.resize_token_embeddings(len(tokenizer))\n\n# 3. Load and Tokenize Dataset\ndataset_path = \"/kaggle/working/keyword_story_dataset_50k.txt\"\nraw_dataset = load_and_preprocess_data(dataset_path)\n# Tokenize the dataset\nprint(\"Tokenize\")\ntokenized_dataset = raw_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n\n# Save the tokenized dataset to disk\ntokenized_dataset.save_to_disk(\"/kaggle/working/tokenized_dataset_50k\")\n\n# Now you can load it later using:\n# from datasets import load_from_disk\n# tokenized_dataset = load_from_disk(\"/kaggle/working/tokenized_dataset\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T05:57:57.959178Z","iopub.execute_input":"2025-02-04T05:57:57.959564Z","iopub.status.idle":"2025-02-04T05:59:15.836958Z","shell.execute_reply.started":"2025-02-04T05:57:57.959523Z","shell.execute_reply":"2025-02-04T05:59:15.836054Z"}},"outputs":[{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"Tokenize\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8414f8bde3fa4f02bdc275f9853dc9e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0123aa712279420a8704f567a6eda302"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from datasets import load_dataset, Dataset, load_from_disk\n# from datasets import load_from_disk\ntokenized_dataset_10k = load_from_disk(\"/kaggle/working/tokenized_dataset_50k\")\n# Split into training and validation\ntokenized_dataset = tokenized_dataset_10k.train_test_split(test_size=0.1)\n\n# 4. Define Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    save_steps=500,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    load_best_model_at_end=True\n)\n\n# 5. Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer\n)\n\n# 6. Train the Model\ntrainer.train()\n\n# 7. Save the Model\nmodel.save_pretrained(\"./fine_tuned_gpt2\")\ntokenizer.save_pretrained(\"./fine_tuned_gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T06:13:21.684934Z","iopub.execute_input":"2025-02-04T06:13:21.685284Z","execution_failed":"2025-02-04T06:22:55.527Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_23/3043713729.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1001' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1001/11250 07:34 < 1:17:44, 2.20 it/s, Epoch 0.09/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.404500</td>\n      <td>0.980985</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.006000</td>\n      <td>0.939133</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"\n# 8. Generate Stories\ndef generate_story(keywords, max_length=200):\n    prompt = f\"Keywords: {keywords}\\nStory:\"\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)  # Move to model's device\n    output = model.generate(\n        input_ids=input_ids,\n        max_length=max_length,\n        num_return_sequences=1,\n        temperature=0.9,\n        top_p=0.9,\n        top_k=50,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n\n# Example: Generate a story\nkeywords = \"magic, dragon, castle\"\nprint(generate_story(keywords))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T12:29:46.926623Z","iopub.execute_input":"2024-12-15T12:29:46.926958Z","iopub.status.idle":"2024-12-15T12:29:48.567911Z","shell.execute_reply.started":"2024-12-15T12:29:46.926926Z","shell.execute_reply":"2024-12-15T12:29:48.566859Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Keywords: magic, dragon, castle\nStory: Once there was a magic dragon. He was very powerful and he was very brave. He was very big and he had a big castle. The castle was very big and it was very tall. The dragon was very brave. He was very strong and he had a big castle. The dragon was very tall and he had a big castle. The dragon was very brave and he had a big castle. The dragon was very strong and he had a big castle. The dragon was very brave and he had a big castle. The dragon was very strong and he had a big castle. The dragon was very strong and he had a big castle. The dragon was very brave and he had a big castle. The dragon was very strong and he had a big castle. The dragon was very strong and he had a big castle. The dragon was very strong and he had a big castle. The dragon was very brave and he had a big castle. The\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# Example: Generate a story\nkeywords = \"child are playing, there is house in field, airoplane is flying in sky,\"\nprint(generate_story(keywords))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T12:29:49.948356Z","iopub.execute_input":"2024-12-15T12:29:49.948709Z","iopub.status.idle":"2024-12-15T12:29:51.481729Z","shell.execute_reply.started":"2024-12-15T12:29:49.948677Z","shell.execute_reply":"2024-12-15T12:29:51.480335Z"}},"outputs":[{"name":"stdout","text":"Keywords: child are playing, there is house in field, airoplane is flying in sky,\nStory: There is a child are playing in the field. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing with aroids. The child is playing\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"keywords = \"A curious cat found a glowing key in the attic, The robot discovered it had a secret mission hidden in its memory,On a rainy night, a letter arrived with no sender,  The ancient tree whispered secrets to anyone who touched it, A young girl woke up to find her reflection missing from the mirror \"\nprint(generate_story(keywords))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T12:29:51.955958Z","iopub.execute_input":"2024-12-15T12:29:51.956620Z","iopub.status.idle":"2024-12-15T12:29:53.109205Z","shell.execute_reply.started":"2024-12-15T12:29:51.956565Z","shell.execute_reply":"2024-12-15T12:29:53.108358Z"}},"outputs":[{"name":"stdout","text":"Keywords: A curious cat found a glowing key in the attic, The robot discovered it had a secret mission hidden in its memory,On a rainy night, a letter arrived with no sender,  The ancient tree whispered secrets to anyone who touched it, A young girl woke up to find her reflection missing from the mirror \nStory: A curious cat found a mysterious key in the attic. The key was a secret mission hidden in its memory.   The cat was curious about the secret mission. It discovered that the key had a secret mission hidden in its memory.   The cat was curious about the secret mission. It was curious about the secret mission.   The cat found the key in the attic and was curious about the secret mission.   The cat was curious about the secret mission. It was curious about the secret mission.   The cat was curious about the secret mission. It was curious about the secret mission.   The cat was curious about the secret mission\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"keywords = \"Moonlight Secret Journey Treasure Whisper Forest Shadow Magic Mystery Adventure\"\nprint(generate_story(keywords))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T12:30:04.536696Z","iopub.execute_input":"2024-12-15T12:30:04.537251Z","iopub.status.idle":"2024-12-15T12:30:06.125234Z","shell.execute_reply.started":"2024-12-15T12:30:04.537218Z","shell.execute_reply":"2024-12-15T12:30:06.124381Z"}},"outputs":[{"name":"stdout","text":"Keywords: Moonlight Secret Journey Treasure Whisper Forest Shadow Magic Mystery Adventure\nStory: The Shadow of the Moon is a magical land. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets. It is full of secrets\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"import os\nimport zipfile\n\ndef create_zip_from_directory(directory_path, output_zip_path):\n    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                arcname = os.path.relpath(file_path, directory_path)  # Preserve directory structure\n                zipf.write(file_path, arcname)\n    print(f\"Zip file created at: {output_zip_path}\")\n\n# Usage\ndirectory_to_zip = '/kaggle/working/tokenized_dataset_50k'\noutput_zip_file = '/kaggle/working/tokenized_dataset_50k.zip'\ncreate_zip_from_directory(directory_to_zip, output_zip_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T12:52:55.984258Z","iopub.execute_input":"2024-12-15T12:52:55.985023Z","iopub.status.idle":"2024-12-15T12:53:09.080010Z","shell.execute_reply.started":"2024-12-15T12:52:55.984989Z","shell.execute_reply":"2024-12-15T12:53:09.079124Z"}},"outputs":[{"name":"stdout","text":"Zip file created at: /kaggle/working/tokenized_dataset_50k.zip\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"## Story Generation2","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset, Dataset\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:33:16.901350Z","iopub.execute_input":"2025-02-17T18:33:16.901606Z","iopub.status.idle":"2025-02-17T18:33:36.421441Z","shell.execute_reply.started":"2025-02-17T18:33:16.901578Z","shell.execute_reply":"2025-02-17T18:33:36.420675Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import re\n\ndef preprocess_data(file_path):\n    # Read and parse the dataset\n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = f.read()\n    # Extract all keyword-story pairs\n    pattern = re.compile(r'<\\|keywords\\|>(.*?)<\\|story\\|>(.*?)<\\|endoftext\\|>', re.DOTALL)\n    matches = pattern.findall(data)\n\n    data_pairs = []\n    \n    i = 0\n    for keywords, story in matches:\n        # Format the input text with special tokens\n        if(i<50000):\n            formatted_text = f\"<|keywords|> {keywords.strip()} <|story|> {story.strip()} <|endoftext|>\"\n            data_pairs.append(formatted_text)\n            i = i+1\n    return data_pairs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:33:40.348867Z","iopub.execute_input":"2025-02-17T18:33:40.349814Z","iopub.status.idle":"2025-02-17T18:33:40.356735Z","shell.execute_reply.started":"2025-02-17T18:33:40.349765Z","shell.execute_reply":"2025-02-17T18:33:40.355792Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\nfrom datasets import Dataset\n\n# Load the GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n# Add a padding token (GPT-2 does not have one by default)\ntokenizer.pad_token = tokenizer.eos_token # Use EOS token as padding\ntokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|keywords|>\", \"<|story|>\", \"<|endoftext|>\"]})\n\n# Load the GPT-2 model\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Load and preprocess the dataset from the file\ndata_pairs = preprocess_data('/kaggle/input/storydataset2/formatted_text.txt')  # Replace with your file path\n\n# Convert it to Hugging Face Dataset format\ntrain_dataset = Dataset.from_dict({'text': data_pairs})\n\ndef tokenize_function(examples):\n    encoding = tokenizer(\n        examples['text'], \n        padding=\"max_length\",  # Ensures uniform length\n        truncation=True, \n        max_length=512, \n        return_tensors=\"pt\"\n    )\n    encoding[\"labels\"] = encoding[\"input_ids\"]  # Use input_ids as labels\n    return encoding\n\n\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:33:43.724140Z","iopub.execute_input":"2025-02-17T18:33:43.724488Z","iopub.status.idle":"2025-02-17T18:35:11.533495Z","shell.execute_reply.started":"2025-02-17T18:33:43.724458Z","shell.execute_reply":"2025-02-17T18:35:11.532687Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a88fe52ec9a487eb069a01ce724f489"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e88b90444cd4463e91222ca7c93d4eeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9603cdc9012344438ac25b493fd26668"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22770b4672e943c7b4e48ed91a04803b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0f9161236bb4be0a5b8fb04ef533bca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ca580521fd54e09bef357ff9cb35bfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99688a4f3c0349608a953f9e686ec5ad"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cfc983c22054e368f3f1edb3e17ef58"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\n\n# Split into training and validation\ntokenized_dataset = train_dataset.train_test_split(test_size=0.1)\n\n# 4. Define Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results-model2\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    save_steps=500,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    load_best_model_at_end=True\n)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator  # Handles padding during training\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:35:11.534790Z","iopub.execute_input":"2025-02-17T18:35:11.535084Z","iopub.status.idle":"2025-02-17T18:35:13.291267Z","shell.execute_reply.started":"2025-02-17T18:35:11.535059Z","shell.execute_reply":"2025-02-17T18:35:13.290548Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_23/3083514734.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Fine-tune the model\ntrainer.train()\n\n# 7. Save the Model\nmodel.save_pretrained(\"./fine_tuned_gpt2\")\ntokenizer.save_pretrained(\"./fine_tuned_gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:38:14.190535Z","iopub.execute_input":"2025-02-17T18:38:14.190909Z","iopub.status.idle":"2025-02-17T18:38:14.195608Z","shell.execute_reply.started":"2025-02-17T18:38:14.190874Z","shell.execute_reply":"2025-02-17T18:38:14.194823Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load the fine-tuned model and tokenizer\nmodel_path = \"./fine_tuned_gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n\n# Ensure model is in evaluation mode\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:17:12.720998Z","iopub.execute_input":"2025-02-05T12:17:12.721939Z","iopub.status.idle":"2025-02-05T12:17:12.879631Z","shell.execute_reply.started":"2025-02-05T12:17:12.721892Z","shell.execute_reply":"2025-02-05T12:17:12.878751Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50259, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"def generate_story(keywords):\n    prompt = f\"<|keywords|>{keywords}<|story|>\"\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n    output = model.generate(\n        input_ids,\n        max_length=300,\n        do_sample=True,\n        temperature=0.9,\n        top_p=0.92,\n        repetition_penalty=1.2,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    story = tokenizer.decode(output[0], skip_special_tokens=False)\n    story = story.split(\"<|story|>\")[1].replace(\"<|endoftext|>\", \"\").strip()\n    return story","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:17:45.466616Z","iopub.execute_input":"2025-02-05T12:17:45.466957Z","iopub.status.idle":"2025-02-05T12:17:45.473000Z","shell.execute_reply.started":"2025-02-05T12:17:45.466927Z","shell.execute_reply":"2025-02-05T12:17:45.472111Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"generate_story(\"day, girl, named, Lily, found, needle, room, knew, play, wanted, share, mom, sew, button, shirt, went\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:17:47.713895Z","iopub.execute_input":"2025-02-05T12:17:47.714363Z","iopub.status.idle":"2025-02-05T12:17:51.456056Z","shell.execute_reply.started":"2025-02-05T12:17:47.714317Z","shell.execute_reply":"2025-02-05T12:17:51.455192Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'One day a little girl named Lily found a needle in her room. She knew it was very special and couldn\\'t play with it because she wanted to share it with her mom. Lily wanted the needle to be clean and shiny. So, she went to her mom and said, \"I can sew a button on my shirt.\" Her mom was proud of her and they both went to put the button on the shirt. From that day onward, Lily knew that if she wanted something clean and pretty, she could share it with her mom.'"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"keywords = \"Moonlight Secret Journey Treasure Whisper Forest Shadow Magic Mystery Adventure\"\nprint(generate_story(keywords))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:18:06.829528Z","iopub.execute_input":"2025-02-05T12:18:06.829863Z","iopub.status.idle":"2025-02-05T12:18:10.817737Z","shell.execute_reply.started":"2025-02-05T12:18:06.829834Z","shell.execute_reply":"2025-02-05T12:18:10.816697Z"}},"outputs":[{"name":"stdout","text":"Dungeon: On Behalf of God, there was a mighty guardian. He had the biggest and strongest will in the whole of the forest - and he was very powerful.  The guardian was so mighty that nobody could ever be able to be trusted. So every day, the guardian would go quietly into the deepest, darkest of dark places. But each time he went through this mysterious, frightening secret, nobody was able to be trusted.  Moral ofThe Dayâ„¢s story is never too safe for you! When you are strong enough?â„¢Å“If only you could trust God.Ã¢â‚¬â„¢\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Example: Generate a story\nkeywords = \"Opnce upon a time, wand, danger dragon , black castle\"\nprint(generate_story(keywords))\nprint(\"==================\")\n# # Example: Generate a story\nkeywords = \"Once upon a time, wand, danger dragon , black castle\"\nprint(generate_story(keywords))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:18:48.806861Z","iopub.execute_input":"2025-02-05T12:18:48.807241Z","iopub.status.idle":"2025-02-05T12:18:55.228462Z","shell.execute_reply.started":"2025-02-05T12:18:48.807207Z","shell.execute_reply":"2025-02-05T12:18:55.227498Z"}},"outputs":[{"name":"stdout","text":"Openceupon was a brave and fierce warrior. He was always ready to fight when it was his turn!  The dragon was very strong and would often go out on the castle. But he was also very brave of himself.  One day, Joe had to face an even more fierce dragon! It was big and strong, like no other dragon in the whole castle.   Joe's bravery against such a fierce dragon made him even stronger than ever before.  Joe is now only one brave warrior but he is still able to do great things with his sword.\n==================\nOnce upon the time there was an evil dragon. The dragon was very dark and scary and could not go anywhere.  He had to go away when it was too dangerous. One day, he was brave enough to go into another castle.  But this time, it was much safer. The dragon was still safe from being in his dark castle.  He was ready for anything!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Step 1: Improved dataset preprocessing\n# def generate_random_pairs(input_file, output_file, num_pairs=50000):\n#     with open(input_file, 'r') as file:\n#         data = file.read().split('<|endoftext|>')\n#     pairs = random.sample[:num_pairs]\n#     output_text = \"\"\n#     for pair in pairs:\n#         if \"Keywords:\" in pair and \"Story:\" in pair:\n#             kw, story = pair.split(\"Story:\", 1)\n#             kw = kw.replace(\"Keywords:\", \"\").strip()\n#             story = story.strip()\n#             output_text += f\"<|keywords|>{kw}<|story|>{story}<|endoftext|>\\n\"\n#     with open(output_file, 'w') as file:\n#         file.write(output_text)\n\n# Step 2: Training with special tokens\n# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n# tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|keywords|>\", \"<|story|>\", \"<|endoftext|>\"]})\n# model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n# model.resize_token_embeddings(len(tokenizer))\n\n# Step 3: Generate with better prompts\n# def generate_story(keywords):\n#     prompt = f\"<|keywords|>{keywords}<|story|>\"\n#     input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n#     output = model.generate(\n#         input_ids,\n#         max_length=300,\n#         do_sample=True,\n#         temperature=0.9,\n#         top_p=0.92,\n#         repetition_penalty=1.2,\n#         pad_token_id=tokenizer.eos_token_id\n#     )\n#     story = tokenizer.decode(output[0], skip_special_tokens=False)\n#     story = story.split(\"<|story|>\")[1].replace(\"<|endoftext|>\", \"\").strip()\n#     return story","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T22:19:46.573721Z","iopub.execute_input":"2025-02-04T22:19:46.574447Z","iopub.status.idle":"2025-02-04T22:19:48.524726Z","shell.execute_reply.started":"2025-02-04T22:19:46.574407Z","shell.execute_reply":"2025-02-04T22:19:48.523974Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# # Example usage:\n# input_file = '/kaggle/input/keyword-story-dataset/keyword_story_dataset.txt'  # Replace with the path to your input file\n# output_file = '/kaggle/working/keyword_story_dataset_50k.txt'  # Replace with the path where you want to save the output\n\n# generate_random_pairs(input_file, output_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T22:19:48.526301Z","iopub.execute_input":"2025-02-04T22:19:48.526939Z","iopub.status.idle":"2025-02-04T22:19:48.531290Z","shell.execute_reply.started":"2025-02-04T22:19:48.526898Z","shell.execute_reply":"2025-02-04T22:19:48.530305Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# generate_story(\"day, girl, named, Lily, found, needle, room, knew, play, wanted, share, mom, sew, button, shirt, went\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keywords = \"An astronaut drifts alone in space, staring at the ruins of an ancient civilization on a forgotten planet, A detective dusts off an old book, revealing a hidden map that could expose a powerful secret society, A lone robot wanders through an abandoned city, searching for signs of the last human survivor\"\nprint(generate_story(keywords))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:21:02.113472Z","iopub.execute_input":"2025-02-05T12:21:02.113831Z","iopub.status.idle":"2025-02-05T12:21:07.587169Z","shell.execute_reply.started":"2025-02-05T12:21:02.113799Z","shell.execute_reply":"2025-02-05T12:21:07.586181Z"}},"outputs":[{"name":"stdout","text":"Once there was An astronaut who wasn't afraid to be brave. He's just staring around at his ruins, staring up at the terrible ruins of an ancient civilization.  Suddenly he sees something shiny and brilliant. It's a tiny map! The astronaut's eyes are glimmering, but he's still scared.  The explorer looks carefully behind the map, until he's out of sight. He's only three-years-old - but now he is strong enough to reveal it once more.  The explorer has revealed the secrets from the ancient city, so all the robots and humans must be brave too. The astronaut is glad he's been brave, and no longer can be fearful. He's also glad he's able Toompose himself with the map as he walks away.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"keywords = \"forensic team, airoplan in sky, child are playing\"\nprint(generate_story(keywords))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:25:00.863745Z","iopub.execute_input":"2025-02-05T12:25:00.864088Z","iopub.status.idle":"2025-02-05T12:25:07.585558Z","shell.execute_reply.started":"2025-02-05T12:25:00.864058Z","shell.execute_reply":"2025-02-05T12:25:07.584788Z"}},"outputs":[{"name":"stdout","text":"Forensic team has a Forensic team. It is very organized. Every child is in the team. The team members are all united in one single operation.  In this operation, they do everything together. They act like doctors, nurses and other special people. They act like the Forensics team and they act like the other children. Everyone is strong and united by their work.  The forensic team have a great time at the same time. They act like the other children and act like the Forensics team and act like the other children. They act like the doctors who act with power. And everyone is always united as the team members act like the Forensics team and act like the other children too!  The Forensic teams act and act for many years. Everywhere they go, everyone is there to act together. They act like the doctors who act with power and act like the Fore Forensic team. And everyone is so proud of them.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!zip -r file_tuned_gpt2.zip '/kaggle/working/fine_tuned_gpt2'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:27:51.232837Z","iopub.execute_input":"2025-02-05T12:27:51.233559Z","iopub.status.idle":"2025-02-05T12:28:17.684657Z","shell.execute_reply.started":"2025-02-05T12:27:51.233524Z","shell.execute_reply":"2025-02-05T12:28:17.683807Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/fine_tuned_gpt2/ (stored 0%)\n  adding: kaggle/working/fine_tuned_gpt2/added_tokens.json (deflated 20%)\n  adding: kaggle/working/fine_tuned_gpt2/config.json (deflated 51%)\n  adding: kaggle/working/fine_tuned_gpt2/tokenizer_config.json (deflated 71%)\n  adding: kaggle/working/fine_tuned_gpt2/model.safetensors (deflated 7%)\n  adding: kaggle/working/fine_tuned_gpt2/vocab.json (deflated 68%)\n  adding: kaggle/working/fine_tuned_gpt2/generation_config.json (deflated 24%)\n  adding: kaggle/working/fine_tuned_gpt2/merges.txt (deflated 53%)\n  adding: kaggle/working/fine_tuned_gpt2/special_tokens_map.json (deflated 81%)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}