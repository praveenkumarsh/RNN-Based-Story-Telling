{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-05T05:19:02.083159Z",
     "iopub.status.busy": "2024-12-05T05:19:02.082697Z",
     "iopub.status.idle": "2024-12-05T05:19:02.113171Z",
     "shell.execute_reply": "2024-12-05T05:19:02.112102Z",
     "shell.execute_reply.started": "2024-12-05T05:19:02.083124Z"
    }
   },
   "source": [
    "## Setup Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T16:57:40.014600Z",
     "iopub.status.busy": "2025-02-21T16:57:40.013654Z",
     "iopub.status.idle": "2025-02-21T16:57:41.202098Z",
     "shell.execute_reply": "2025-02-21T16:57:41.200430Z",
     "shell.execute_reply.started": "2025-02-21T16:57:40.014561Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/kaggle/working/': Device or resource busy\n"
     ]
    }
   ],
   "source": [
    "!rm -r /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:30:18.593962Z",
     "iopub.status.busy": "2025-02-22T12:30:18.593744Z",
     "iopub.status.idle": "2025-02-22T12:30:18.824947Z",
     "shell.execute_reply": "2025-02-22T12:30:18.823858Z",
     "shell.execute_reply.started": "2025-02-22T12:30:18.593941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./models’: File exists\n",
      "mkdir: cannot create directory ‘./data’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./models\n",
    "!mkdir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:30:23.245506Z",
     "iopub.status.busy": "2025-02-22T12:30:23.245193Z",
     "iopub.status.idle": "2025-02-22T12:30:27.452137Z",
     "shell.execute_reply": "2025-02-22T12:30:27.451270Z",
     "shell.execute_reply.started": "2025-02-22T12:30:23.245480Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools~=2.0.4 in /usr/local/lib/python3.10/dist-packages (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools~=2.0.4) (3.7.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools~=2.0.4) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->pycocotools~=2.0.4) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->pycocotools~=2.0.4) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->pycocotools~=2.0.4) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->pycocotools~=2.0.4) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->pycocotools~=2.0.4) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->pycocotools~=2.0.4) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools~=2.0.4) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pycocotools~=2.0.4) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pycocotools~=2.0.4) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->pycocotools~=2.0.4) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->pycocotools~=2.0.4) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->pycocotools~=2.0.4) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools~=2.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Declare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:30:27.687408Z",
     "iopub.status.busy": "2025-02-22T12:30:27.687049Z",
     "iopub.status.idle": "2025-02-22T12:30:27.691524Z",
     "shell.execute_reply": "2025-02-22T12:30:27.690689Z",
     "shell.execute_reply.started": "2025-02-22T12:30:27.687375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "coco_dataset_path=\"/kaggle/input/coco-2017-dataset\"\n",
    "coco2017_train_img_path = \"coco2017/train2017/\"\n",
    "coco2017_test_img_path = \"coco2017/test2017/\"\n",
    "coco2017_val_img_path = \"coco2017/val2017/\"\n",
    "coco2017_train_annotation_path = \"coco2017/annotations/captions_train2017.json\"\n",
    "coco2017_test_annotation_path = \"coco2017/annotations/image_info_test2017.json\"\n",
    "coco2017_val_annotation_path = \"coco2017/annotations/captions_val2017.json\"\n",
    "\n",
    "vocab_file=\"/kaggle/working/data/vocab.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:30:31.121805Z",
     "iopub.status.busy": "2025-02-22T12:30:31.121496Z",
     "iopub.status.idle": "2025-02-22T12:30:32.843803Z",
     "shell.execute_reply": "2025-02-22T12:30:32.843119Z",
     "shell.execute_reply.started": "2025-02-22T12:30:31.121781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "annotations_file = os.path.join(\n",
    "    coco_dataset_path, coco2017_train_annotation_path\n",
    ")\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_threshold,\n",
    "        vocab_file=vocab_file,\n",
    "        start_word=\"<start>\",\n",
    "        end_word=\"<end>\",\n",
    "        unk_word=\"<unk>\",\n",
    "        annotations_file=annotations_file,\n",
    "        vocab_from_file=False,\n",
    "    ):\n",
    "        self.vocab_threshold = vocab_threshold\n",
    "        self.vocab_file = vocab_file\n",
    "        self.start_word = start_word\n",
    "        self.end_word = end_word\n",
    "        self.unk_word = unk_word\n",
    "        self.annotations_file = annotations_file\n",
    "        self.vocab_from_file = vocab_from_file\n",
    "        self.get_vocab()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        if os.path.exists(self.vocab_file) and self.vocab_from_file:\n",
    "            with open(self.vocab_file, \"rb\") as f:\n",
    "                print(\"self.vocab_file : \", self.vocab_file)\n",
    "                print(\"file :\",f)\n",
    "                vocab = pickle.load(f)\n",
    "            self.word2idx = vocab.word2idx\n",
    "            self.idx2word = vocab.idx2word\n",
    "            print(\"Vocabulary successfully loaded from vocab.pkl file!\")\n",
    "\n",
    "        # create a new vocab file\n",
    "        else:\n",
    "            self.build_vocab()\n",
    "            with open(self.vocab_file, \"wb\") as f:\n",
    "                pickle.dump(self, f)\n",
    "\n",
    "    def build_vocab(self):\n",
    "        self.init_vocab()\n",
    "        self.add_word(self.start_word)\n",
    "        self.add_word(self.end_word)\n",
    "        self.add_word(self.unk_word)\n",
    "        self.add_captions()\n",
    "\n",
    "    def init_vocab(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def add_captions(self):\n",
    "        coco = COCO(self.annotations_file)\n",
    "        counter = Counter()\n",
    "        ids = coco.anns.keys()\n",
    "        for i, idx in enumerate(ids):\n",
    "            caption = str(coco.anns[idx][\"caption\"])\n",
    "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "            counter.update(tokens)\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
    "                \n",
    "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx[self.unk_word]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:30:33.973210Z",
     "iopub.status.busy": "2025-02-22T12:30:33.972801Z",
     "iopub.status.idle": "2025-02-22T12:30:37.248189Z",
     "shell.execute_reply": "2025-02-22T12:30:37.247344Z",
     "shell.execute_reply.started": "2025-02-22T12:30:33.973185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_loader(\n",
    "    transform,\n",
    "    mode=\"train\",\n",
    "    batch_size=1,\n",
    "    vocab_threshold=None,\n",
    "    vocab_file=vocab_file,\n",
    "    start_word=\"<start>\",\n",
    "    end_word=\"<end>\",\n",
    "    unk_word=\"<unk>\",\n",
    "    vocab_from_file=True,\n",
    "    num_workers=0,\n",
    "    cocoapi_loc=coco_dataset_path,\n",
    "):\n",
    "    if mode == \"train\":\n",
    "        img_folder = os.path.join(cocoapi_loc, coco2017_train_img_path)\n",
    "        annotations_file = os.path.join(\n",
    "            cocoapi_loc, coco2017_train_annotation_path\n",
    "        )\n",
    "    elif mode == \"valid\":\n",
    "        img_folder = os.path.join(cocoapi_loc, coco2017_val_img_path)\n",
    "        annotations_file = os.path.join(\n",
    "            cocoapi_loc, coco2017_val_annotation_path\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}\")\n",
    "    # COCO caption dataset.\n",
    "    dataset = CoCoDataset(\n",
    "        transform=transform,\n",
    "        mode=mode,\n",
    "        batch_size=batch_size,\n",
    "        vocab_threshold=vocab_threshold,\n",
    "        vocab_file=vocab_file,\n",
    "        start_word=start_word,\n",
    "        end_word=end_word,\n",
    "        unk_word=unk_word,\n",
    "        annotations_file=annotations_file,\n",
    "        vocab_from_file=vocab_from_file,\n",
    "        img_folder=img_folder,\n",
    "    )\n",
    "\n",
    "    if mode == \"train\":\n",
    "        indices = dataset.get_train_indices()\n",
    "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        # data loader for COCO dataset.\n",
    "        data_loader = data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_sampler=data.sampler.BatchSampler(\n",
    "                sampler=initial_sampler, batch_size=dataset.batch_size, drop_last=False\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        data_loader = data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=dataset.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "class CoCoDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform,\n",
    "        mode,\n",
    "        batch_size,\n",
    "        vocab_threshold,\n",
    "        vocab_file,\n",
    "        start_word,\n",
    "        end_word,\n",
    "        unk_word,\n",
    "        annotations_file,\n",
    "        vocab_from_file,\n",
    "        img_folder,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab = Vocabulary(\n",
    "            vocab_threshold,\n",
    "            vocab_file,\n",
    "            start_word,\n",
    "            end_word,\n",
    "            unk_word,\n",
    "            annotations_file,\n",
    "            vocab_from_file,\n",
    "        )\n",
    "        self.img_folder = img_folder\n",
    "        if self.mode == \"train\":\n",
    "            self.coco = COCO(annotations_file)\n",
    "            self.ids = list(self.coco.anns.keys())\n",
    "            print(\"Obtaining caption lengths...\")\n",
    "            all_tokens = [\n",
    "                nltk.tokenize.word_tokenize(\n",
    "                    str(self.coco.anns[self.ids[index]][\"caption\"]).lower()\n",
    "                )\n",
    "                for index in tqdm(np.arange(len(self.ids)))\n",
    "            ]\n",
    "            self.caption_lengths = [len(token) for token in all_tokens]\n",
    "        else:\n",
    "            test_info = json.loads(open(annotations_file).read())\n",
    "            self.paths = [item[\"file_name\"] for item in test_info[\"images\"]]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # obtain image and caption if in training mode\n",
    "        if self.mode == \"train\":\n",
    "            ann_id = self.ids[index]\n",
    "            caption = self.coco.anns[ann_id][\"caption\"]\n",
    "            img_id = self.coco.anns[ann_id][\"image_id\"]\n",
    "            path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # Convert caption to tensor of word ids.\n",
    "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "            caption = []\n",
    "            caption.append(self.vocab(self.vocab.start_word))\n",
    "            caption.extend([self.vocab(token) for token in tokens])\n",
    "            caption.append(self.vocab(self.vocab.end_word))\n",
    "            caption = torch.Tensor(caption).long()\n",
    "\n",
    "            # return pre-processed image and caption tensors\n",
    "            return image, caption\n",
    "\n",
    "        elif self.mode == \"valid\":\n",
    "            path = self.paths[index]\n",
    "            image_id = int(path.split(\"/\")[0].split(\".\")[0].split(\"_\")[-1])\n",
    "            pil_image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            image = self.transform(pil_image)\n",
    "\n",
    "            # return original image and pre-processed image tensor\n",
    "            return image_id, image\n",
    "        # obtain image if in test mode\n",
    "        else:\n",
    "            path = self.paths[index]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            pil_image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            orig_image = np.array(pil_image)\n",
    "            image = self.transform(pil_image)\n",
    "\n",
    "            # return original image and pre-processed image tensor\n",
    "            return orig_image, image\n",
    "\n",
    "    def get_train_indices(self):\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "        all_indices = np.where(\n",
    "            [\n",
    "                self.caption_lengths[i] == sel_length\n",
    "                for i in np.arange(len(self.caption_lengths))\n",
    "            ]\n",
    "        )[0]\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == \"train\":\n",
    "            return len(self.ids)\n",
    "        else:\n",
    "            return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:30:38.248500Z",
     "iopub.status.busy": "2025-02-22T12:30:38.247997Z",
     "iopub.status.idle": "2025-02-22T12:30:42.598766Z",
     "shell.execute_reply": "2025-02-22T12:30:42.597809Z",
     "shell.execute_reply.started": "2025-02-22T12:30:38.248467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:31:31.070876Z",
     "iopub.status.busy": "2025-02-22T12:31:31.070549Z",
     "iopub.status.idle": "2025-02-22T12:31:31.107400Z",
     "shell.execute_reply": "2025-02-22T12:31:31.106590Z",
     "shell.execute_reply.started": "2025-02-22T12:31:31.070850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # batch size\n",
    "vocab_threshold = 2  # minimum word count threshold\n",
    "vocab_from_file = False  # if True, load existing vocab file\n",
    "embed_size = 1024  # dimensionality of image and word embeddings\n",
    "hidden_size = 2048  # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 5  # number of training epochs\n",
    "save_every = 1  # determines frequency of saving model weights\n",
    "print_every = 20  # determines window for printing average loss\n",
    "log_file = \"training_log.txt\"  # name of file with saved training loss and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:31:42.412143Z",
     "iopub.status.busy": "2025-02-22T12:31:42.411841Z",
     "iopub.status.idle": "2025-02-22T12:31:42.451540Z",
     "shell.execute_reply": "2025-02-22T12:31:42.450564Z",
     "shell.execute_reply.started": "2025-02-22T12:31:42.412122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:31:44.541620Z",
     "iopub.status.busy": "2025-02-22T12:31:44.541218Z",
     "iopub.status.idle": "2025-02-22T12:31:44.865778Z",
     "shell.execute_reply": "2025-02-22T12:31:44.865077Z",
     "shell.execute_reply.started": "2025-02-22T12:31:44.541591Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:31:48.026863Z",
     "iopub.status.busy": "2025-02-22T12:31:48.026523Z",
     "iopub.status.idle": "2025-02-22T12:33:46.944035Z",
     "shell.execute_reply": "2025-02-22T12:33:46.942794Z",
     "shell.execute_reply.started": "2025-02-22T12:31:48.026834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.84s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/591753] Tokenizing captions...\n",
      "[100000/591753] Tokenizing captions...\n",
      "[200000/591753] Tokenizing captions...\n",
      "[300000/591753] Tokenizing captions...\n",
      "[400000/591753] Tokenizing captions...\n",
      "[500000/591753] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.95s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002dc0bc88c244ffaedafce459c00f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591753 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build data loader.\n",
    "data_loader = get_loader(\n",
    "    transform=transform_train,\n",
    "    mode=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_from_file=vocab_from_file,\n",
    "    cocoapi_loc=coco_dataset_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:33:46.945830Z",
     "iopub.status.busy": "2025-02-22T12:33:46.945565Z",
     "iopub.status.idle": "2025-02-22T12:33:46.990586Z",
     "shell.execute_reply": "2025-02-22T12:33:46.989705Z",
     "shell.execute_reply.started": "2025-02-22T12:33:46.945810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# ----------- Encoder ------------\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # disable learning for parameters\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "# --------- Decoder ----------\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Assigning hidden dimension\n",
    "        self.hidden_dim = hidden_size\n",
    "        # Map each word index to a dense word embedding tensor of embed_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        # Creating LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Initializing linear to apply at last of RNN layer for further prediction\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # Initializing values for hidden and cell state\n",
    "        self.hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # remove <end> token from captions and embed captions\n",
    "        cap_embedding = self.embed(\n",
    "            captions[:, :-1]\n",
    "        )  # (bs, cap_length) -> (bs, cap_length-1, embed_size)\n",
    "\n",
    "        embeddings = torch.cat((features.unsqueeze(dim=1), cap_embedding), dim=1)\n",
    "\n",
    "        #  getting output i.e. score and hidden layer.\n",
    "        # first value: all the hidden states throughout the sequence. second value: the most recent hidden state\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeddings\n",
    "        )  # (bs, cap_length, hidden_size), (1, bs, hidden_size)\n",
    "        outputs = self.linear(lstm_out)  # (bs, cap_length, vocab_size)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        res = []\n",
    "\n",
    "        # Now we feed the LSTM output and hidden states back into itself to get the caption\n",
    "        for i in range(max_len):\n",
    "            lstm_out, states = self.lstm(\n",
    "                inputs, states\n",
    "            )  # lstm_out: (1, 1, hidden_size)\n",
    "            outputs = self.linear(lstm_out.squeeze(dim=1))  # outputs: (1, vocab_size)\n",
    "            _, predicted_idx = outputs.max(dim=1)  # predicted: (1, 1)\n",
    "            res.append(predicted_idx.item())\n",
    "            # if the predicted idx is the stop index, the loop stops\n",
    "            if predicted_idx == 1:\n",
    "                break\n",
    "            inputs = self.embed(predicted_idx)  # inputs: (1, embed_size)\n",
    "            # prepare input for next iteration\n",
    "            inputs = inputs.unsqueeze(1)  # inputs: (1, 1, embed_size)\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Step Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T13:26:42.819275Z",
     "iopub.status.busy": "2025-02-22T13:26:42.818969Z",
     "iopub.status.idle": "2025-02-22T13:26:44.015694Z",
     "shell.execute_reply": "2025-02-22T13:26:44.014909Z",
     "shell.execute_reply.started": "2025-02-22T13:26:42.819251Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is :  16807\n"
     ]
    }
   ],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(\"vocab size is : \",vocab_size)\n",
    "\n",
    "# Initializing the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Defining the loss function\n",
    "criterion = (\n",
    "    nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    ")\n",
    "\n",
    "# Specifying the learnable parameters of the mode\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Defining the optimize\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoc\n",
    "total_step = math.ceil(len(data_loader.dataset) / data_loader.batch_sampler.batch_size)\n",
    "\n",
    "# Define the scheduler (reduces LR every epoch)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T13:26:46.244490Z",
     "iopub.status.busy": "2025-02-22T13:26:46.244091Z",
     "iopub.status.idle": "2025-02-22T13:26:46.287672Z",
     "shell.execute_reply": "2025-02-22T13:26:46.286857Z",
     "shell.execute_reply.started": "2025-02-22T13:26:46.244457Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Device Name:  Tesla P100-PCIE-16GB\n",
      "Memory Usage:\n",
      "Allocated: 1.2 GB\n",
      "Cached:    7.2 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(\"Device Name: \", torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Outputs in Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T23:45:45.038023Z",
     "iopub.status.busy": "2025-02-22T23:45:45.037744Z",
     "iopub.status.idle": "2025-02-22T23:47:05.204402Z",
     "shell.execute_reply": "2025-02-22T23:47:05.203649Z",
     "shell.execute_reply.started": "2025-02-22T23:45:45.038001Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/ImageCapionCoCoResults_models.zip'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"ImageCapionCoCoResults_models\", 'zip', \"/kaggle/working/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T23:48:00.378289Z",
     "iopub.status.busy": "2025-02-22T23:48:00.377977Z",
     "iopub.status.idle": "2025-02-22T23:48:00.443915Z",
     "shell.execute_reply": "2025-02-22T23:48:00.443271Z",
     "shell.execute_reply.started": "2025-02-22T23:48:00.378265Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/ImageCapionCoCoResults_data.zip'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"ImageCapionCoCoResults_data\", 'zip', \"/kaggle/working/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T23:48:02.535446Z",
     "iopub.status.busy": "2025-02-22T23:48:02.535099Z",
     "iopub.status.idle": "2025-02-22T23:48:04.157685Z",
     "shell.execute_reply": "2025-02-22T23:48:04.156949Z",
     "shell.execute_reply.started": "2025-02-22T23:48:02.535416Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.vocab_file :  /kaggle/working/data/vocab.pkl\n",
      "file : <_io.BufferedReader name='/kaggle/working/data/vocab.pkl'>\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-6f07cb04c7f9>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(os.path.join(\"./models\", encoder_file)))\n",
      "<ipython-input-26-6f07cb04c7f9>:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load(os.path.join(\"./models\", decoder_file)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(16807, 1024)\n",
       "  (lstm): LSTM(1024, 2048, batch_first=True)\n",
       "  (linear): Linear(in_features=2048, out_features=16807, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "#Create the data loader.\n",
    "val_data_loader = get_loader(\n",
    "    transform=transform_test, mode=\"valid\", cocoapi_loc=coco_dataset_path\n",
    ")\n",
    "\n",
    "\n",
    "encoder_file = \"encoder-4.pkl\"\n",
    "decoder_file = \"decoder-4.pkl\"\n",
    "\n",
    "# Initialize the encoder and decoder.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Moving models to GPU if CUDA is available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Loading the trained weights\n",
    "encoder.load_state_dict(torch.load(os.path.join(\"./models\", encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join(\"./models\", decoder_file)))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T23:48:07.225352Z",
     "iopub.status.busy": "2025-02-22T23:48:07.225023Z",
     "iopub.status.idle": "2025-02-22T23:48:07.262102Z",
     "shell.execute_reply": "2025-02-22T23:48:07.261191Z",
     "shell.execute_reply.started": "2025-02-22T23:48:07.225293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "def clean_sentence(output, idx2word):\n",
    "    sentence = \"\"\n",
    "    for i in output:\n",
    "        word = idx2word[i]\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == 1:\n",
    "            break\n",
    "        if i == 18:\n",
    "            sentence = sentence + word\n",
    "        else:\n",
    "            sentence = sentence + \" \" + word\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def bleu_score(true_sentences, predicted_sentences):\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    for img_id in set(true_sentences.keys()).intersection(\n",
    "        set(predicted_sentences.keys())\n",
    "    ):\n",
    "        img_refs = [cap.split() for cap in true_sentences[img_id]]\n",
    "        references.append(img_refs)\n",
    "        hypotheses.append(predicted_sentences[img_id][0].strip().split())\n",
    "\n",
    "    return corpus_bleu(references, hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T23:48:10.456118Z",
     "iopub.status.busy": "2025-02-22T23:48:10.455806Z",
     "iopub.status.idle": "2025-02-22T23:50:54.907524Z",
     "shell.execute_reply": "2025-02-22T23:50:54.906404Z",
     "shell.execute_reply.started": "2025-02-22T23:48:10.456092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be56f0573214bccb52decdb3cc2df93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# infer captions for all images\n",
    "pred_result = defaultdict(list)\n",
    "for img_id, img in tqdm(val_data_loader):\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = encoder(img).unsqueeze(1)\n",
    "        output = decoder.sample(features)\n",
    "    sentence = clean_sentence(output, val_data_loader.dataset.vocab.idx2word)\n",
    "    pred_result[img_id.item()].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T23:52:24.674505Z",
     "iopub.status.busy": "2025-02-22T23:52:24.674155Z",
     "iopub.status.idle": "2025-02-22T23:52:24.751094Z",
     "shell.execute_reply": "2025-02-22T23:52:24.750483Z",
     "shell.execute_reply.started": "2025-02-22T23:52:24.674481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\n",
    "    #os.path.join(cocoapi_dir, \"cocoapi\", \"annotations/captions_val2014.json\"), \"r\"\n",
    "    os.path.join(coco_dataset_path, coco2017_val_annotation_path), \"r\"\n",
    ") as f:\n",
    "    caption = json.load(f)\n",
    "\n",
    "valid_annot = caption[\"annotations\"]\n",
    "valid_result = defaultdict(list)\n",
    "for i in valid_annot:\n",
    "    valid_result[i[\"image_id\"]].append(i[\"caption\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T23:52:26.308569Z",
     "iopub.status.busy": "2025-02-22T23:52:26.308252Z",
     "iopub.status.idle": "2025-02-22T23:52:26.345323Z",
     "shell.execute_reply": "2025-02-22T23:52:26.344400Z",
     "shell.execute_reply.started": "2025-02-22T23:52:26.308546Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a black honda motorcycle parked in front of a garage.',\n",
       "  'a honda motorcycle parked in a grass driveway',\n",
       "  'a black honda motorcycle with a dark burgundy seat.',\n",
       "  'ma motorcycle parked on the gravel in front of a garage',\n",
       "  'a motorcycle with its brake extended standing outside'],\n",
       " ['an office cubicle with four different types of computers.',\n",
       "  'the home office space seems to be very cluttered.',\n",
       "  'an office with desk computer and chair and laptop.',\n",
       "  'office setting with a lot of computer screens.',\n",
       "  'a desk and chair in an office cubicle.'],\n",
       " ['a small closed toilet in a cramped space.',\n",
       "  'a tan toilet and sink combination in a small room.',\n",
       "  'this is an advanced toilet with a sink and control panel.',\n",
       "  'a close-up picture of a toilet with a fountain.',\n",
       "  'off white toilet with a faucet and controls. ']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(valid_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T23:52:28.631076Z",
     "iopub.status.busy": "2025-02-22T23:52:28.630793Z",
     "iopub.status.idle": "2025-02-22T23:52:28.668209Z",
     "shell.execute_reply": "2025-02-22T23:52:28.667360Z",
     "shell.execute_reply.started": "2025-02-22T23:52:28.631057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' a street with a red stop sign on the side of the road .'],\n",
       " [' a group of people sitting at tables under umbrellas .'],\n",
       " [' a city street filled with lots of traffic .']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pred_result.values())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bleu Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T23:52:44.266938Z",
     "iopub.status.busy": "2025-02-22T23:52:44.266651Z",
     "iopub.status.idle": "2025-02-22T23:52:45.262558Z",
     "shell.execute_reply": "2025-02-22T23:52:45.261777Z",
     "shell.execute_reply.started": "2025-02-22T23:52:44.266917Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17277985715938968"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score(true_sentences=valid_result, predicted_sentences=pred_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
