{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-05T05:19:02.083159Z",
     "iopub.status.busy": "2024-12-05T05:19:02.082697Z",
     "iopub.status.idle": "2024-12-05T05:19:02.113171Z",
     "shell.execute_reply": "2024-12-05T05:19:02.112102Z",
     "shell.execute_reply.started": "2024-12-05T05:19:02.083124Z"
    }
   },
   "source": [
    "## Setup Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir ./models\n",
    "!mkdir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pycocotools~=2.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Declare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "coco_dataset_path=\"/kaggle/input/coco-2017-dataset\"\n",
    "coco2017_train_img_path = \"coco2017/train2017/\"\n",
    "coco2017_test_img_path = \"coco2017/test2017/\"\n",
    "coco2017_val_img_path = \"coco2017/val2017/\"\n",
    "coco2017_train_annotation_path = \"coco2017/annotations/captions_train2017.json\"\n",
    "coco2017_test_annotation_path = \"coco2017/annotations/image_info_test2017.json\"\n",
    "coco2017_val_annotation_path = \"coco2017/annotations/captions_val2017.json\"\n",
    "\n",
    "vocab_file=\"/kaggle/working/data/vocab.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "annotations_file = os.path.join(\n",
    "    coco_dataset_path, coco2017_train_annotation_path\n",
    ")\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_threshold,\n",
    "        vocab_file=vocab_file,\n",
    "        start_word=\"<start>\",\n",
    "        end_word=\"<end>\",\n",
    "        unk_word=\"<unk>\",\n",
    "        annotations_file=annotations_file,\n",
    "        vocab_from_file=False,\n",
    "    ):\n",
    "        self.vocab_threshold = vocab_threshold\n",
    "        self.vocab_file = vocab_file\n",
    "        self.start_word = start_word\n",
    "        self.end_word = end_word\n",
    "        self.unk_word = unk_word\n",
    "        self.annotations_file = annotations_file\n",
    "        self.vocab_from_file = vocab_from_file\n",
    "        self.get_vocab()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        if os.path.exists(self.vocab_file) and self.vocab_from_file:\n",
    "            with open(self.vocab_file, \"rb\") as f:\n",
    "                print(\"self.vocab_file : \", self.vocab_file)\n",
    "                print(\"file :\",f)\n",
    "                vocab = pickle.load(f)\n",
    "            self.word2idx = vocab.word2idx\n",
    "            self.idx2word = vocab.idx2word\n",
    "            print(\"Vocabulary successfully loaded from vocab.pkl file!\")\n",
    "\n",
    "        # create a new vocab file\n",
    "        else:\n",
    "            self.build_vocab()\n",
    "            with open(self.vocab_file, \"wb\") as f:\n",
    "                pickle.dump(self, f)\n",
    "\n",
    "    def build_vocab(self):\n",
    "        self.init_vocab()\n",
    "        self.add_word(self.start_word)\n",
    "        self.add_word(self.end_word)\n",
    "        self.add_word(self.unk_word)\n",
    "        self.add_captions()\n",
    "\n",
    "    def init_vocab(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def add_captions(self):\n",
    "        coco = COCO(self.annotations_file)\n",
    "        counter = Counter()\n",
    "        ids = coco.anns.keys()\n",
    "        for i, idx in enumerate(ids):\n",
    "            caption = str(coco.anns[idx][\"caption\"])\n",
    "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "            counter.update(tokens)\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
    "                \n",
    "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx[self.unk_word]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_loader(\n",
    "    transform,\n",
    "    mode=\"train\",\n",
    "    batch_size=1,\n",
    "    vocab_threshold=None,\n",
    "    vocab_file=vocab_file,\n",
    "    start_word=\"<start>\",\n",
    "    end_word=\"<end>\",\n",
    "    unk_word=\"<unk>\",\n",
    "    vocab_from_file=True,\n",
    "    num_workers=0,\n",
    "    cocoapi_loc=coco_dataset_path,\n",
    "):\n",
    "    if mode == \"train\":\n",
    "        img_folder = os.path.join(cocoapi_loc, coco2017_train_img_path)\n",
    "        annotations_file = os.path.join(\n",
    "            cocoapi_loc, coco2017_train_annotation_path\n",
    "        )\n",
    "    elif mode == \"valid\":\n",
    "        img_folder = os.path.join(cocoapi_loc, coco2017_val_img_path)\n",
    "        annotations_file = os.path.join(\n",
    "            cocoapi_loc, coco2017_val_annotation_path\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}\")\n",
    "    # COCO caption dataset.\n",
    "    dataset = CoCoDataset(\n",
    "        transform=transform,\n",
    "        mode=mode,\n",
    "        batch_size=batch_size,\n",
    "        vocab_threshold=vocab_threshold,\n",
    "        vocab_file=vocab_file,\n",
    "        start_word=start_word,\n",
    "        end_word=end_word,\n",
    "        unk_word=unk_word,\n",
    "        annotations_file=annotations_file,\n",
    "        vocab_from_file=vocab_from_file,\n",
    "        img_folder=img_folder,\n",
    "    )\n",
    "\n",
    "    if mode == \"train\":\n",
    "        indices = dataset.get_train_indices()\n",
    "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        # data loader for COCO dataset.\n",
    "        data_loader = data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_sampler=data.sampler.BatchSampler(\n",
    "                sampler=initial_sampler, batch_size=dataset.batch_size, drop_last=False\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        data_loader = data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=dataset.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "class CoCoDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform,\n",
    "        mode,\n",
    "        batch_size,\n",
    "        vocab_threshold,\n",
    "        vocab_file,\n",
    "        start_word,\n",
    "        end_word,\n",
    "        unk_word,\n",
    "        annotations_file,\n",
    "        vocab_from_file,\n",
    "        img_folder,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab = Vocabulary(\n",
    "            vocab_threshold,\n",
    "            vocab_file,\n",
    "            start_word,\n",
    "            end_word,\n",
    "            unk_word,\n",
    "            annotations_file,\n",
    "            vocab_from_file,\n",
    "        )\n",
    "        self.img_folder = img_folder\n",
    "        if self.mode == \"train\":\n",
    "            self.coco = COCO(annotations_file)\n",
    "            self.ids = list(self.coco.anns.keys())\n",
    "            print(\"Obtaining caption lengths...\")\n",
    "            all_tokens = [\n",
    "                nltk.tokenize.word_tokenize(\n",
    "                    str(self.coco.anns[self.ids[index]][\"caption\"]).lower()\n",
    "                )\n",
    "                for index in tqdm(np.arange(len(self.ids)))\n",
    "            ]\n",
    "            self.caption_lengths = [len(token) for token in all_tokens]\n",
    "        else:\n",
    "            test_info = json.loads(open(annotations_file).read())\n",
    "            self.paths = [item[\"file_name\"] for item in test_info[\"images\"]]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # obtain image and caption if in training mode\n",
    "        if self.mode == \"train\":\n",
    "            ann_id = self.ids[index]\n",
    "            caption = self.coco.anns[ann_id][\"caption\"]\n",
    "            img_id = self.coco.anns[ann_id][\"image_id\"]\n",
    "            path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # Convert caption to tensor of word ids.\n",
    "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "            caption = []\n",
    "            caption.append(self.vocab(self.vocab.start_word))\n",
    "            caption.extend([self.vocab(token) for token in tokens])\n",
    "            caption.append(self.vocab(self.vocab.end_word))\n",
    "            caption = torch.Tensor(caption).long()\n",
    "\n",
    "            # return pre-processed image and caption tensors\n",
    "            return image, caption\n",
    "\n",
    "        elif self.mode == \"valid\":\n",
    "            path = self.paths[index]\n",
    "            image_id = int(path.split(\"/\")[0].split(\".\")[0].split(\"_\")[-1])\n",
    "            pil_image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            image = self.transform(pil_image)\n",
    "\n",
    "            # return original image and pre-processed image tensor\n",
    "            return image_id, image\n",
    "        # obtain image if in test mode\n",
    "        else:\n",
    "            path = self.paths[index]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            pil_image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            orig_image = np.array(pil_image)\n",
    "            image = self.transform(pil_image)\n",
    "\n",
    "            # return original image and pre-processed image tensor\n",
    "            return orig_image, image\n",
    "\n",
    "    def get_train_indices(self):\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "        all_indices = np.where(\n",
    "            [\n",
    "                self.caption_lengths[i] == sel_length\n",
    "                for i in np.arange(len(self.caption_lengths))\n",
    "            ]\n",
    "        )[0]\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == \"train\":\n",
    "            return len(self.ids)\n",
    "        else:\n",
    "            return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T16:37:38.167651Z",
     "iopub.status.busy": "2025-02-02T16:37:38.166718Z",
     "iopub.status.idle": "2025-02-02T16:37:38.202918Z",
     "shell.execute_reply": "2025-02-02T16:37:38.202025Z",
     "shell.execute_reply.started": "2025-02-02T16:37:38.167616Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # batch size\n",
    "vocab_threshold = 4  # minimum word count threshold\n",
    "vocab_from_file = False  # if True, load existing vocab file\n",
    "embed_size = 256  # dimensionality of image and word embeddings\n",
    "hidden_size = 512  # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 5  # number of training epochs\n",
    "save_every = 1  # determines frequency of saving model weights\n",
    "print_every = 20  # determines window for printing average loss\n",
    "log_file = \"training_log.txt\"  # name of file with saved training loss and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T16:37:40.165679Z",
     "iopub.status.busy": "2025-02-02T16:37:40.164871Z",
     "iopub.status.idle": "2025-02-02T16:37:40.201591Z",
     "shell.execute_reply": "2025-02-02T16:37:40.200841Z",
     "shell.execute_reply.started": "2025-02-02T16:37:40.165641Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        # smaller edge of image resized to 256\n",
    "        transforms.Resize(256),\n",
    "        # get 224x224 crop from random location\n",
    "        transforms.RandomCrop(224),\n",
    "        # horizontally flip image with probability=0.5\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # convert the PIL Image to a tensor\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T16:37:43.446385Z",
     "iopub.status.busy": "2025-02-02T16:37:43.446018Z",
     "iopub.status.idle": "2025-02-02T16:39:33.413741Z",
     "shell.execute_reply": "2025-02-02T16:39:33.413029Z",
     "shell.execute_reply.started": "2025-02-02T16:37:43.446357Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.68s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/591753] Tokenizing captions...\n",
      "[100000/591753] Tokenizing captions...\n",
      "[200000/591753] Tokenizing captions...\n",
      "[300000/591753] Tokenizing captions...\n",
      "[400000/591753] Tokenizing captions...\n",
      "[500000/591753] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=1.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bea5bcf7a047449d3cd0d3b55a96ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591753 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build data loader.\n",
    "data_loader = get_loader(\n",
    "    transform=transform_train,\n",
    "    mode=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_from_file=vocab_from_file,\n",
    "    cocoapi_loc=coco_dataset_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T19:09:08.621215Z",
     "iopub.status.busy": "2025-02-02T19:09:08.620854Z",
     "iopub.status.idle": "2025-02-02T19:09:08.662510Z",
     "shell.execute_reply": "2025-02-02T19:09:08.661612Z",
     "shell.execute_reply.started": "2025-02-02T19:09:08.621185Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# ----------- Encoder ------------\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # disable learning for parameters\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "# --------- Decoder ----------\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Assigning hidden dimension\n",
    "        self.hidden_dim = hidden_size\n",
    "        # Map each word index to a dense word embedding tensor of embed_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        # Creating LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Initializing linear to apply at last of RNN layer for further prediction\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # Initializing values for hidden and cell state\n",
    "        self.hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # remove <end> token from captions and embed captions\n",
    "        cap_embedding = self.embed(\n",
    "            captions[:, :-1]\n",
    "        )  # (bs, cap_length) -> (bs, cap_length-1, embed_size)\n",
    "\n",
    "        embeddings = torch.cat((features.unsqueeze(dim=1), cap_embedding), dim=1)\n",
    "\n",
    "        #  getting output i.e. score and hidden layer.\n",
    "        # first value: all the hidden states throughout the sequence. second value: the most recent hidden state\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeddings\n",
    "        )  # (bs, cap_length, hidden_size), (1, bs, hidden_size)\n",
    "        outputs = self.linear(lstm_out)  # (bs, cap_length, vocab_size)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        res = []\n",
    "\n",
    "        # Now we feed the LSTM output and hidden states back into itself to get the caption\n",
    "        for i in range(max_len):\n",
    "            lstm_out, states = self.lstm(\n",
    "                inputs, states\n",
    "            )  # lstm_out: (1, 1, hidden_size)\n",
    "            outputs = self.linear(lstm_out.squeeze(dim=1))  # outputs: (1, vocab_size)\n",
    "            _, predicted_idx = outputs.max(dim=1)  # predicted: (1, 1)\n",
    "            res.append(predicted_idx.item())\n",
    "            # if the predicted idx is the stop index, the loop stops\n",
    "            if predicted_idx == 1:\n",
    "                break\n",
    "            inputs = self.embed(predicted_idx)  # inputs: (1, embed_size)\n",
    "            # prepare input for next iteration\n",
    "            inputs = inputs.unsqueeze(1)  # inputs: (1, 1, embed_size)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Step Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T16:40:29.635114Z",
     "iopub.status.busy": "2025-02-02T16:40:29.634543Z",
     "iopub.status.idle": "2025-02-02T16:40:31.323096Z",
     "shell.execute_reply": "2025-02-02T16:40:31.321990Z",
     "shell.execute_reply.started": "2025-02-02T16:40:29.635069Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is :  11559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 185MB/s]\n"
     ]
    }
   ],
   "source": [
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(\"vocab size is : \",vocab_size)\n",
    "\n",
    "# Initializing the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Defining the loss function\n",
    "criterion = (\n",
    "    nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    ")\n",
    "\n",
    "# Specifying the learnable parameters of the mode\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Defining the optimize\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoc\n",
    "total_step = math.ceil(len(data_loader.dataset) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T19:10:25.157861Z",
     "iopub.status.busy": "2025-02-02T19:10:25.157307Z",
     "iopub.status.idle": "2025-02-02T19:10:25.194994Z",
     "shell.execute_reply": "2025-02-02T19:10:25.194068Z",
     "shell.execute_reply.started": "2025-02-02T19:10:25.157829Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Device Name:  Tesla P100-PCIE-16GB\n",
      "Memory Usage:\n",
      "Allocated: 0.5 GB\n",
      "Cached:    2.2 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(\"Device Name: \", torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T16:41:25.057384Z",
     "iopub.status.busy": "2025-02-02T16:41:25.056550Z",
     "iopub.status.idle": "2025-02-02T18:48:22.147579Z",
     "shell.execute_reply": "2025-02-02T18:48:22.146609Z",
     "shell.execute_reply.started": "2025-02-02T16:41:25.057349Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [20/4624], Loss: 4.7613, Perplexity: 116.8944\n",
      "Epoch [1/1], Step [40/4624], Loss: 4.4094, Perplexity: 82.2194\n",
      "Epoch [1/1], Step [60/4624], Loss: 4.3515, Perplexity: 77.5958\n",
      "Epoch [1/1], Step [80/4624], Loss: 3.9542, Perplexity: 52.1555\n",
      "Epoch [1/1], Step [100/4624], Loss: 3.9567, Perplexity: 52.2857\n",
      "Epoch [1/1], Step [120/4624], Loss: 3.9220, Perplexity: 50.5003\n",
      "Epoch [1/1], Step [140/4624], Loss: 3.6559, Perplexity: 38.7016\n",
      "Epoch [1/1], Step [160/4624], Loss: 3.5917, Perplexity: 36.2961\n",
      "Epoch [1/1], Step [180/4624], Loss: 3.6178, Perplexity: 37.2548\n",
      "Epoch [1/1], Step [200/4624], Loss: 3.5866, Perplexity: 36.1118\n",
      "Epoch [1/1], Step [220/4624], Loss: 3.5121, Perplexity: 33.5179\n",
      "Epoch [1/1], Step [240/4624], Loss: 3.4225, Perplexity: 30.6449\n",
      "Epoch [1/1], Step [260/4624], Loss: 3.3935, Perplexity: 29.7699\n",
      "Epoch [1/1], Step [280/4624], Loss: 3.4570, Perplexity: 31.7206\n",
      "Epoch [1/1], Step [300/4624], Loss: 4.1586, Perplexity: 63.9817\n",
      "Epoch [1/1], Step [320/4624], Loss: 3.3333, Perplexity: 28.0303\n",
      "Epoch [1/1], Step [340/4624], Loss: 3.2064, Perplexity: 24.6904\n",
      "Epoch [1/1], Step [360/4624], Loss: 3.1749, Perplexity: 23.9251\n",
      "Epoch [1/1], Step [380/4624], Loss: 3.1567, Perplexity: 23.4936\n",
      "Epoch [1/1], Step [400/4624], Loss: 3.8696, Perplexity: 47.9227\n",
      "Epoch [1/1], Step [420/4624], Loss: 3.1095, Perplexity: 22.4088\n",
      "Epoch [1/1], Step [440/4624], Loss: 3.0717, Perplexity: 21.5775\n",
      "Epoch [1/1], Step [460/4624], Loss: 3.2066, Perplexity: 24.6953\n",
      "Epoch [1/1], Step [480/4624], Loss: 3.0316, Perplexity: 20.7299\n",
      "Epoch [1/1], Step [500/4624], Loss: 3.0638, Perplexity: 21.4081\n",
      "Epoch [1/1], Step [520/4624], Loss: 3.0107, Perplexity: 20.3025\n",
      "Epoch [1/1], Step [540/4624], Loss: 2.7941, Perplexity: 16.3473\n",
      "Epoch [1/1], Step [560/4624], Loss: 2.9143, Perplexity: 18.4355\n",
      "Epoch [1/1], Step [580/4624], Loss: 3.0417, Perplexity: 20.9402\n",
      "Epoch [1/1], Step [600/4624], Loss: 3.0695, Perplexity: 21.5313\n",
      "Epoch [1/1], Step [620/4624], Loss: 3.1274, Perplexity: 22.8135\n",
      "Epoch [1/1], Step [640/4624], Loss: 2.9413, Perplexity: 18.9403\n",
      "Epoch [1/1], Step [660/4624], Loss: 2.8903, Perplexity: 17.9980\n",
      "Epoch [1/1], Step [680/4624], Loss: 3.1518, Perplexity: 23.3782\n",
      "Epoch [1/1], Step [700/4624], Loss: 2.9097, Perplexity: 18.3516\n",
      "Epoch [1/1], Step [720/4624], Loss: 2.8320, Perplexity: 16.9788\n",
      "Epoch [1/1], Step [740/4624], Loss: 3.0638, Perplexity: 21.4091\n",
      "Epoch [1/1], Step [760/4624], Loss: 2.7899, Perplexity: 16.2790\n",
      "Epoch [1/1], Step [780/4624], Loss: 3.7120, Perplexity: 40.9374\n",
      "Epoch [1/1], Step [800/4624], Loss: 3.0229, Perplexity: 20.5510\n",
      "Epoch [1/1], Step [820/4624], Loss: 3.0857, Perplexity: 21.8834\n",
      "Epoch [1/1], Step [840/4624], Loss: 2.6623, Perplexity: 14.3289\n",
      "Epoch [1/1], Step [860/4624], Loss: 2.6769, Perplexity: 14.5393\n",
      "Epoch [1/1], Step [880/4624], Loss: 2.7100, Perplexity: 15.0296\n",
      "Epoch [1/1], Step [900/4624], Loss: 3.1773, Perplexity: 23.9819\n",
      "Epoch [1/1], Step [920/4624], Loss: 2.6466, Perplexity: 14.1067\n",
      "Epoch [1/1], Step [940/4624], Loss: 2.7653, Perplexity: 15.8843\n",
      "Epoch [1/1], Step [960/4624], Loss: 2.6765, Perplexity: 14.5337\n",
      "Epoch [1/1], Step [980/4624], Loss: 2.7118, Perplexity: 15.0560\n",
      "Epoch [1/1], Step [1000/4624], Loss: 2.5871, Perplexity: 13.2909\n",
      "Epoch [1/1], Step [1020/4624], Loss: 2.9999, Perplexity: 20.0828\n",
      "Epoch [1/1], Step [1040/4624], Loss: 2.5901, Perplexity: 13.3309\n",
      "Epoch [1/1], Step [1060/4624], Loss: 2.9527, Perplexity: 19.1569\n",
      "Epoch [1/1], Step [1080/4624], Loss: 2.6245, Perplexity: 13.7976\n",
      "Epoch [1/1], Step [1100/4624], Loss: 2.6824, Perplexity: 14.6204\n",
      "Epoch [1/1], Step [1120/4624], Loss: 2.6695, Perplexity: 14.4332\n",
      "Epoch [1/1], Step [1140/4624], Loss: 2.6440, Perplexity: 14.0691\n",
      "Epoch [1/1], Step [1160/4624], Loss: 2.8492, Perplexity: 17.2745\n",
      "Epoch [1/1], Step [1180/4624], Loss: 3.4306, Perplexity: 30.8943\n",
      "Epoch [1/1], Step [1200/4624], Loss: 2.5256, Perplexity: 12.4981\n",
      "Epoch [1/1], Step [1220/4624], Loss: 2.4236, Perplexity: 11.2861\n",
      "Epoch [1/1], Step [1240/4624], Loss: 2.7501, Perplexity: 15.6448\n",
      "Epoch [1/1], Step [1260/4624], Loss: 2.8870, Perplexity: 17.9397\n",
      "Epoch [1/1], Step [1280/4624], Loss: 2.5232, Perplexity: 12.4684\n",
      "Epoch [1/1], Step [1300/4624], Loss: 2.4806, Perplexity: 11.9486\n",
      "Epoch [1/1], Step [1320/4624], Loss: 2.5262, Perplexity: 12.5064\n",
      "Epoch [1/1], Step [1340/4624], Loss: 2.7559, Perplexity: 15.7351\n",
      "Epoch [1/1], Step [1360/4624], Loss: 2.5946, Perplexity: 13.3908\n",
      "Epoch [1/1], Step [1380/4624], Loss: 2.4734, Perplexity: 11.8627\n",
      "Epoch [1/1], Step [1400/4624], Loss: 2.6019, Perplexity: 13.4887\n",
      "Epoch [1/1], Step [1420/4624], Loss: 2.4670, Perplexity: 11.7871\n",
      "Epoch [1/1], Step [1440/4624], Loss: 2.5042, Perplexity: 12.2341\n",
      "Epoch [1/1], Step [1460/4624], Loss: 2.4653, Perplexity: 11.7668\n",
      "Epoch [1/1], Step [1480/4624], Loss: 2.4849, Perplexity: 12.0005\n",
      "Epoch [1/1], Step [1500/4624], Loss: 2.9095, Perplexity: 18.3470\n",
      "Epoch [1/1], Step [1520/4624], Loss: 2.5101, Perplexity: 12.3057\n",
      "Epoch [1/1], Step [1540/4624], Loss: 2.5524, Perplexity: 12.8374\n",
      "Epoch [1/1], Step [1560/4624], Loss: 2.4194, Perplexity: 11.2392\n",
      "Epoch [1/1], Step [1580/4624], Loss: 2.5346, Perplexity: 12.6116\n",
      "Epoch [1/1], Step [1600/4624], Loss: 2.4647, Perplexity: 11.7603\n",
      "Epoch [1/1], Step [1620/4624], Loss: 2.4421, Perplexity: 11.4969\n",
      "Epoch [1/1], Step [1640/4624], Loss: 2.3793, Perplexity: 10.7976\n",
      "Epoch [1/1], Step [1660/4624], Loss: 2.3030, Perplexity: 10.0046\n",
      "Epoch [1/1], Step [1680/4624], Loss: 2.3601, Perplexity: 10.5918\n",
      "Epoch [1/1], Step [1700/4624], Loss: 2.2842, Perplexity: 9.8178\n",
      "Epoch [1/1], Step [1720/4624], Loss: 2.4478, Perplexity: 11.5632\n",
      "Epoch [1/1], Step [1740/4624], Loss: 2.4094, Perplexity: 11.1274\n",
      "Epoch [1/1], Step [1760/4624], Loss: 2.7185, Perplexity: 15.1581\n",
      "Epoch [1/1], Step [1780/4624], Loss: 2.4345, Perplexity: 11.4103\n",
      "Epoch [1/1], Step [1800/4624], Loss: 2.4790, Perplexity: 11.9290\n",
      "Epoch [1/1], Step [1820/4624], Loss: 2.4533, Perplexity: 11.6263\n",
      "Epoch [1/1], Step [1840/4624], Loss: 2.3618, Perplexity: 10.6102\n",
      "Epoch [1/1], Step [1860/4624], Loss: 2.2984, Perplexity: 9.9584\n",
      "Epoch [1/1], Step [1880/4624], Loss: 2.3906, Perplexity: 10.9203\n",
      "Epoch [1/1], Step [1900/4624], Loss: 2.9053, Perplexity: 18.2712\n",
      "Epoch [1/1], Step [1920/4624], Loss: 3.4049, Perplexity: 30.1112\n",
      "Epoch [1/1], Step [1940/4624], Loss: 2.2450, Perplexity: 9.4407\n",
      "Epoch [1/1], Step [1960/4624], Loss: 2.4131, Perplexity: 11.1681\n",
      "Epoch [1/1], Step [1980/4624], Loss: 2.1449, Perplexity: 8.5415\n",
      "Epoch [1/1], Step [2000/4624], Loss: 2.6108, Perplexity: 13.6105\n",
      "Epoch [1/1], Step [2020/4624], Loss: 2.4190, Perplexity: 11.2343\n",
      "Epoch [1/1], Step [2040/4624], Loss: 2.4148, Perplexity: 11.1877\n",
      "Epoch [1/1], Step [2060/4624], Loss: 2.4264, Perplexity: 11.3177\n",
      "Epoch [1/1], Step [2080/4624], Loss: 2.3195, Perplexity: 10.1703\n",
      "Epoch [1/1], Step [2100/4624], Loss: 2.4069, Perplexity: 11.0999\n",
      "Epoch [1/1], Step [2120/4624], Loss: 2.3697, Perplexity: 10.6940\n",
      "Epoch [1/1], Step [2140/4624], Loss: 2.2666, Perplexity: 9.6468\n",
      "Epoch [1/1], Step [2160/4624], Loss: 2.8830, Perplexity: 17.8674\n",
      "Epoch [1/1], Step [2180/4624], Loss: 2.5533, Perplexity: 12.8499\n",
      "Epoch [1/1], Step [2200/4624], Loss: 2.1827, Perplexity: 8.8701\n",
      "Epoch [1/1], Step [2220/4624], Loss: 2.3925, Perplexity: 10.9413\n",
      "Epoch [1/1], Step [2240/4624], Loss: 2.6125, Perplexity: 13.6325\n",
      "Epoch [1/1], Step [2260/4624], Loss: 2.1823, Perplexity: 8.8671\n",
      "Epoch [1/1], Step [2280/4624], Loss: 2.2447, Perplexity: 9.4372\n",
      "Epoch [1/1], Step [2300/4624], Loss: 2.8293, Perplexity: 16.9334\n",
      "Epoch [1/1], Step [2320/4624], Loss: 2.4011, Perplexity: 11.0352\n",
      "Epoch [1/1], Step [2340/4624], Loss: 2.3251, Perplexity: 10.2279\n",
      "Epoch [1/1], Step [2360/4624], Loss: 2.6472, Perplexity: 14.1144\n",
      "Epoch [1/1], Step [2380/4624], Loss: 2.3201, Perplexity: 10.1769\n",
      "Epoch [1/1], Step [2400/4624], Loss: 2.8488, Perplexity: 17.2679\n",
      "Epoch [1/1], Step [2420/4624], Loss: 2.3524, Perplexity: 10.5103\n",
      "Epoch [1/1], Step [2440/4624], Loss: 2.4033, Perplexity: 11.0595\n",
      "Epoch [1/1], Step [2460/4624], Loss: 2.2785, Perplexity: 9.7623\n",
      "Epoch [1/1], Step [2480/4624], Loss: 3.0760, Perplexity: 21.6723\n",
      "Epoch [1/1], Step [2500/4624], Loss: 2.3536, Perplexity: 10.5232\n",
      "Epoch [1/1], Step [2520/4624], Loss: 2.3987, Perplexity: 11.0092\n",
      "Epoch [1/1], Step [2540/4624], Loss: 2.3357, Perplexity: 10.3364\n",
      "Epoch [1/1], Step [2560/4624], Loss: 2.8284, Perplexity: 16.9179\n",
      "Epoch [1/1], Step [2580/4624], Loss: 2.2345, Perplexity: 9.3414\n",
      "Epoch [1/1], Step [2600/4624], Loss: 2.2023, Perplexity: 9.0462\n",
      "Epoch [1/1], Step [2620/4624], Loss: 2.0992, Perplexity: 8.1596\n",
      "Epoch [1/1], Step [2640/4624], Loss: 2.1494, Perplexity: 8.5796\n",
      "Epoch [1/1], Step [2660/4624], Loss: 2.6174, Perplexity: 13.7005\n",
      "Epoch [1/1], Step [2680/4624], Loss: 2.2857, Perplexity: 9.8328\n",
      "Epoch [1/1], Step [2700/4624], Loss: 2.1465, Perplexity: 8.5548\n",
      "Epoch [1/1], Step [2720/4624], Loss: 2.1301, Perplexity: 8.4157\n",
      "Epoch [1/1], Step [2740/4624], Loss: 2.4720, Perplexity: 11.8456\n",
      "Epoch [1/1], Step [2760/4624], Loss: 2.3207, Perplexity: 10.1828\n",
      "Epoch [1/1], Step [2780/4624], Loss: 2.2822, Perplexity: 9.7984\n",
      "Epoch [1/1], Step [2800/4624], Loss: 2.3928, Perplexity: 10.9446\n",
      "Epoch [1/1], Step [2820/4624], Loss: 2.2330, Perplexity: 9.3276\n",
      "Epoch [1/1], Step [2840/4624], Loss: 2.3368, Perplexity: 10.3482\n",
      "Epoch [1/1], Step [2860/4624], Loss: 2.2425, Perplexity: 9.4170\n",
      "Epoch [1/1], Step [2880/4624], Loss: 2.1655, Perplexity: 8.7188\n",
      "Epoch [1/1], Step [2900/4624], Loss: 2.3287, Perplexity: 10.2641\n",
      "Epoch [1/1], Step [2920/4624], Loss: 2.1075, Perplexity: 8.2273\n",
      "Epoch [1/1], Step [2940/4624], Loss: 2.1413, Perplexity: 8.5102\n",
      "Epoch [1/1], Step [2960/4624], Loss: 2.3207, Perplexity: 10.1824\n",
      "Epoch [1/1], Step [2980/4624], Loss: 2.2194, Perplexity: 9.2020\n",
      "Epoch [1/1], Step [3000/4624], Loss: 2.3998, Perplexity: 11.0213\n",
      "Epoch [1/1], Step [3020/4624], Loss: 2.1627, Perplexity: 8.6947\n",
      "Epoch [1/1], Step [3040/4624], Loss: 2.6198, Perplexity: 13.7332\n",
      "Epoch [1/1], Step [3060/4624], Loss: 2.2244, Perplexity: 9.2479\n",
      "Epoch [1/1], Step [3080/4624], Loss: 2.1777, Perplexity: 8.8260\n",
      "Epoch [1/1], Step [3100/4624], Loss: 2.0293, Perplexity: 7.6086\n",
      "Epoch [1/1], Step [3120/4624], Loss: 2.1352, Perplexity: 8.4587\n",
      "Epoch [1/1], Step [3140/4624], Loss: 2.2355, Perplexity: 9.3515\n",
      "Epoch [1/1], Step [3160/4624], Loss: 2.1651, Perplexity: 8.7158\n",
      "Epoch [1/1], Step [3180/4624], Loss: 2.4409, Perplexity: 11.4831\n",
      "Epoch [1/1], Step [3200/4624], Loss: 2.0493, Perplexity: 7.7624\n",
      "Epoch [1/1], Step [3220/4624], Loss: 2.4272, Perplexity: 11.3268\n",
      "Epoch [1/1], Step [3240/4624], Loss: 2.3474, Perplexity: 10.4578\n",
      "Epoch [1/1], Step [3260/4624], Loss: 2.1911, Perplexity: 8.9448\n",
      "Epoch [1/1], Step [3280/4624], Loss: 2.0124, Perplexity: 7.4811\n",
      "Epoch [1/1], Step [3300/4624], Loss: 2.9358, Perplexity: 18.8373\n",
      "Epoch [1/1], Step [3320/4624], Loss: 2.1727, Perplexity: 8.7819\n",
      "Epoch [1/1], Step [3340/4624], Loss: 2.3714, Perplexity: 10.7126\n",
      "Epoch [1/1], Step [3360/4624], Loss: 2.2360, Perplexity: 9.3556\n",
      "Epoch [1/1], Step [3380/4624], Loss: 2.5063, Perplexity: 12.2594\n",
      "Epoch [1/1], Step [3400/4624], Loss: 2.0533, Perplexity: 7.7934\n",
      "Epoch [1/1], Step [3420/4624], Loss: 2.0617, Perplexity: 7.8590\n",
      "Epoch [1/1], Step [3440/4624], Loss: 2.0760, Perplexity: 7.9728\n",
      "Epoch [1/1], Step [3460/4624], Loss: 2.0833, Perplexity: 8.0311\n",
      "Epoch [1/1], Step [3480/4624], Loss: 2.4785, Perplexity: 11.9235\n",
      "Epoch [1/1], Step [3500/4624], Loss: 2.1162, Perplexity: 8.2998\n",
      "Epoch [1/1], Step [3520/4624], Loss: 2.0833, Perplexity: 8.0310\n",
      "Epoch [1/1], Step [3540/4624], Loss: 2.1351, Perplexity: 8.4583\n",
      "Epoch [1/1], Step [3560/4624], Loss: 2.1451, Perplexity: 8.5432\n",
      "Epoch [1/1], Step [3580/4624], Loss: 2.2230, Perplexity: 9.2349\n",
      "Epoch [1/1], Step [3600/4624], Loss: 2.1341, Perplexity: 8.4494\n",
      "Epoch [1/1], Step [3620/4624], Loss: 2.1639, Perplexity: 8.7051\n",
      "Epoch [1/1], Step [3640/4624], Loss: 2.1357, Perplexity: 8.4630\n",
      "Epoch [1/1], Step [3660/4624], Loss: 4.6091, Perplexity: 100.3907\n",
      "Epoch [1/1], Step [3680/4624], Loss: 2.1466, Perplexity: 8.5555\n",
      "Epoch [1/1], Step [3700/4624], Loss: 2.1764, Perplexity: 8.8142\n",
      "Epoch [1/1], Step [3720/4624], Loss: 2.0379, Perplexity: 7.6748\n",
      "Epoch [1/1], Step [3740/4624], Loss: 2.8617, Perplexity: 17.4913\n",
      "Epoch [1/1], Step [3760/4624], Loss: 2.5441, Perplexity: 12.7313\n",
      "Epoch [1/1], Step [3780/4624], Loss: 2.1312, Perplexity: 8.4251\n",
      "Epoch [1/1], Step [3800/4624], Loss: 2.2553, Perplexity: 9.5385\n",
      "Epoch [1/1], Step [3820/4624], Loss: 2.1581, Perplexity: 8.6543\n",
      "Epoch [1/1], Step [3840/4624], Loss: 2.2725, Perplexity: 9.7037\n",
      "Epoch [1/1], Step [3860/4624], Loss: 2.0812, Perplexity: 8.0140\n",
      "Epoch [1/1], Step [3880/4624], Loss: 2.1748, Perplexity: 8.8008\n",
      "Epoch [1/1], Step [3900/4624], Loss: 2.1934, Perplexity: 8.9653\n",
      "Epoch [1/1], Step [3920/4624], Loss: 2.1759, Perplexity: 8.8102\n",
      "Epoch [1/1], Step [3940/4624], Loss: 1.9883, Perplexity: 7.3033\n",
      "Epoch [1/1], Step [3960/4624], Loss: 2.1700, Perplexity: 8.7585\n",
      "Epoch [1/1], Step [3980/4624], Loss: 2.3137, Perplexity: 10.1116\n",
      "Epoch [1/1], Step [4000/4624], Loss: 2.2773, Perplexity: 9.7500\n",
      "Epoch [1/1], Step [4020/4624], Loss: 2.1184, Perplexity: 8.3178\n",
      "Epoch [1/1], Step [4040/4624], Loss: 2.3789, Perplexity: 10.7935\n",
      "Epoch [1/1], Step [4060/4624], Loss: 2.0728, Perplexity: 7.9468\n",
      "Epoch [1/1], Step [4080/4624], Loss: 2.7007, Perplexity: 14.8904\n",
      "Epoch [1/1], Step [4100/4624], Loss: 2.0355, Perplexity: 7.6558\n",
      "Epoch [1/1], Step [4120/4624], Loss: 1.9851, Perplexity: 7.2800\n",
      "Epoch [1/1], Step [4140/4624], Loss: 2.3514, Perplexity: 10.4998\n",
      "Epoch [1/1], Step [4160/4624], Loss: 2.2709, Perplexity: 9.6879\n",
      "Epoch [1/1], Step [4180/4624], Loss: 2.0029, Perplexity: 7.4105\n",
      "Epoch [1/1], Step [4200/4624], Loss: 1.9985, Perplexity: 7.3779\n",
      "Epoch [1/1], Step [4220/4624], Loss: 1.9976, Perplexity: 7.3716\n",
      "Epoch [1/1], Step [4240/4624], Loss: 2.5175, Perplexity: 12.3976\n",
      "Epoch [1/1], Step [4260/4624], Loss: 2.1093, Perplexity: 8.2421\n",
      "Epoch [1/1], Step [4280/4624], Loss: 2.1308, Perplexity: 8.4213\n",
      "Epoch [1/1], Step [4300/4624], Loss: 2.4124, Perplexity: 11.1604\n",
      "Epoch [1/1], Step [4320/4624], Loss: 2.0290, Perplexity: 7.6066\n",
      "Epoch [1/1], Step [4340/4624], Loss: 2.0490, Perplexity: 7.7599\n",
      "Epoch [1/1], Step [4360/4624], Loss: 2.1246, Perplexity: 8.3694\n",
      "Epoch [1/1], Step [4380/4624], Loss: 2.0979, Perplexity: 8.1490\n",
      "Epoch [1/1], Step [4400/4624], Loss: 2.0330, Perplexity: 7.6373\n",
      "Epoch [1/1], Step [4420/4624], Loss: 2.1444, Perplexity: 8.5371\n",
      "Epoch [1/1], Step [4440/4624], Loss: 2.2371, Perplexity: 9.3662\n",
      "Epoch [1/1], Step [4460/4624], Loss: 2.7776, Perplexity: 16.0810\n",
      "Epoch [1/1], Step [4480/4624], Loss: 2.3764, Perplexity: 10.7659\n",
      "Epoch [1/1], Step [4500/4624], Loss: 2.1154, Perplexity: 8.2925\n",
      "Epoch [1/1], Step [4520/4624], Loss: 2.7560, Perplexity: 15.7366\n",
      "Epoch [1/1], Step [4540/4624], Loss: 2.1176, Perplexity: 8.3113\n",
      "Epoch [1/1], Step [4560/4624], Loss: 2.4776, Perplexity: 11.9123\n",
      "Epoch [1/1], Step [4580/4624], Loss: 2.1857, Perplexity: 8.8965\n",
      "Epoch [1/1], Step [4600/4624], Loss: 2.1276, Perplexity: 8.3946\n",
      "Epoch [1/1], Step [4620/4624], Loss: 2.1142, Perplexity: 8.2826\n"
     ]
    }
   ],
   "source": [
    "# Open the training log file.\n",
    "f = open(log_file, \"w\")\n",
    "num_epochs = 1\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for i_step in range(1, total_step + 1):\n",
    "\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # Passing the inputs through the CNN-RNN model\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculating the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "        # Backwarding pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Getting training statistics\n",
    "        stats = (\n",
    "            f\"Epoch [{epoch}/{num_epochs}], Step [{i_step}/{total_step}], \"\n",
    "            f\"Loss: {loss.item():.4f}, Perplexity: {np.exp(loss.item()):.4f}\"\n",
    "        )\n",
    "\n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print(\"\\r\" + stats)\n",
    "\n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(\n",
    "            decoder.state_dict(), os.path.join(\"./models\", \"decoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "        torch.save(\n",
    "            encoder.state_dict(), os.path.join(\"./models\", \"encoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Outputs in Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T18:52:00.654264Z",
     "iopub.status.busy": "2025-02-02T18:52:00.653938Z",
     "iopub.status.idle": "2025-02-02T18:52:35.370341Z",
     "shell.execute_reply": "2025-02-02T18:52:35.369443Z",
     "shell.execute_reply.started": "2025-02-02T18:52:00.654237Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/ImageCapionCoCoResults_models.zip'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"ImageCapionCoCoResults_models\", 'zip', \"/kaggle/working/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T18:52:37.823001Z",
     "iopub.status.busy": "2025-02-02T18:52:37.822344Z",
     "iopub.status.idle": "2025-02-02T18:52:37.878498Z",
     "shell.execute_reply": "2025-02-02T18:52:37.877823Z",
     "shell.execute_reply.started": "2025-02-02T18:52:37.822968Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/ImageCapionCoCoResults_data.zip'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"ImageCapionCoCoResults_data\", 'zip', \"/kaggle/working/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T18:54:51.348359Z",
     "iopub.status.busy": "2025-02-02T18:54:51.348016Z",
     "iopub.status.idle": "2025-02-02T18:54:52.195512Z",
     "shell.execute_reply": "2025-02-02T18:54:52.194650Z",
     "shell.execute_reply.started": "2025-02-02T18:54:51.348328Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.vocab_file :  /kaggle/working/data/vocab.pkl\n",
      "file : <_io.BufferedReader name='/kaggle/working/data/vocab.pkl'>\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/808384892.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(os.path.join(\"./models\", encoder_file)))\n",
      "/tmp/ipykernel_23/808384892.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load(os.path.join(\"./models\", decoder_file)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(11559, 256)\n",
       "  (lstm): LSTM(256, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=11559, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "\n",
    "            \n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Create the data loader.\n",
    "val_data_loader = get_loader(\n",
    "    transform=transform_test, mode=\"valid\", cocoapi_loc=coco_dataset_path\n",
    ")\n",
    "\n",
    "\n",
    "encoder_file = \"encoder-3.pkl\"\n",
    "decoder_file = \"decoder-3.pkl\"\n",
    "\n",
    "# Initialize the encoder and decoder.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Moving models to GPU if CUDA is available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Loading the trained weights\n",
    "encoder.load_state_dict(torch.load(os.path.join(\"./models\", encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join(\"./models\", decoder_file)))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T19:06:08.494406Z",
     "iopub.status.busy": "2025-02-02T19:06:08.493501Z",
     "iopub.status.idle": "2025-02-02T19:06:08.532032Z",
     "shell.execute_reply": "2025-02-02T19:06:08.531157Z",
     "shell.execute_reply.started": "2025-02-02T19:06:08.494368Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "def clean_sentence(output, idx2word):\n",
    "    sentence = \"\"\n",
    "    for i in output:\n",
    "        word = idx2word[i]\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == 1:\n",
    "            break\n",
    "        if i == 18:\n",
    "            sentence = sentence + word\n",
    "        else:\n",
    "            sentence = sentence + \" \" + word\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def bleu_score(true_sentences, predicted_sentences):\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    for img_id in set(true_sentences.keys()).intersection(\n",
    "        set(predicted_sentences.keys())\n",
    "    ):\n",
    "        img_refs = [cap.split() for cap in true_sentences[img_id]]\n",
    "        references.append(img_refs)\n",
    "        hypotheses.append(predicted_sentences[img_id][0].strip().split())\n",
    "\n",
    "    return corpus_bleu(references, hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T18:55:51.518820Z",
     "iopub.status.busy": "2025-02-02T18:55:51.517981Z",
     "iopub.status.idle": "2025-02-02T18:58:15.343419Z",
     "shell.execute_reply": "2025-02-02T18:58:15.342576Z",
     "shell.execute_reply.started": "2025-02-02T18:55:51.518783Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:23<00:00, 34.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# infer captions for all images\n",
    "pred_result = defaultdict(list)\n",
    "for img_id, img in tqdm(val_data_loader):\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = encoder(img).unsqueeze(1)\n",
    "        output = decoder.sample(features)\n",
    "    sentence = clean_sentence(output, val_data_loader.dataset.vocab.idx2word)\n",
    "    pred_result[img_id.item()].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T19:04:53.635415Z",
     "iopub.status.busy": "2025-02-02T19:04:53.635063Z",
     "iopub.status.idle": "2025-02-02T19:04:53.735968Z",
     "shell.execute_reply": "2025-02-02T19:04:53.735048Z",
     "shell.execute_reply.started": "2025-02-02T19:04:53.635388Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\n",
    "    #os.path.join(cocoapi_dir, \"cocoapi\", \"annotations/captions_val2014.json\"), \"r\"\n",
    "    os.path.join(coco_dataset_path, coco2017_val_annotation_path), \"r\"\n",
    ") as f:\n",
    "    caption = json.load(f)\n",
    "\n",
    "valid_annot = caption[\"annotations\"]\n",
    "valid_result = defaultdict(list)\n",
    "for i in valid_annot:\n",
    "    valid_result[i[\"image_id\"]].append(i[\"caption\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T19:04:59.565599Z",
     "iopub.status.busy": "2025-02-02T19:04:59.565009Z",
     "iopub.status.idle": "2025-02-02T19:04:59.601988Z",
     "shell.execute_reply": "2025-02-02T19:04:59.601184Z",
     "shell.execute_reply.started": "2025-02-02T19:04:59.565565Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a black honda motorcycle parked in front of a garage.',\n",
       "  'a honda motorcycle parked in a grass driveway',\n",
       "  'a black honda motorcycle with a dark burgundy seat.',\n",
       "  'ma motorcycle parked on the gravel in front of a garage',\n",
       "  'a motorcycle with its brake extended standing outside'],\n",
       " ['an office cubicle with four different types of computers.',\n",
       "  'the home office space seems to be very cluttered.',\n",
       "  'an office with desk computer and chair and laptop.',\n",
       "  'office setting with a lot of computer screens.',\n",
       "  'a desk and chair in an office cubicle.'],\n",
       " ['a small closed toilet in a cramped space.',\n",
       "  'a tan toilet and sink combination in a small room.',\n",
       "  'this is an advanced toilet with a sink and control panel.',\n",
       "  'a close-up picture of a toilet with a fountain.',\n",
       "  'off white toilet with a faucet and controls. ']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(valid_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T19:05:27.492819Z",
     "iopub.status.busy": "2025-02-02T19:05:27.492483Z",
     "iopub.status.idle": "2025-02-02T19:05:27.529509Z",
     "shell.execute_reply": "2025-02-02T19:05:27.528575Z",
     "shell.execute_reply.started": "2025-02-02T19:05:27.492789Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' a large group of people standing outside of a store .'],\n",
       " [' a pizza sitting on a table with a slice missing .'],\n",
       " [' a man riding a horse on a dirt road .']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pred_result.values())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bleu Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T19:05:33.211396Z",
     "iopub.status.busy": "2025-02-02T19:05:33.210751Z",
     "iopub.status.idle": "2025-02-02T19:05:34.171144Z",
     "shell.execute_reply": "2025-02-02T19:05:34.170273Z",
     "shell.execute_reply.started": "2025-02-02T19:05:33.211364Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19269854822999372"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score(true_sentences=valid_result, predicted_sentences=pred_result)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
