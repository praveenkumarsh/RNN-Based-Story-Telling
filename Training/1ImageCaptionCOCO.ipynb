{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-05T05:19:02.083159Z",
     "iopub.status.busy": "2024-12-05T05:19:02.082697Z",
     "iopub.status.idle": "2024-12-05T05:19:02.113171Z",
     "shell.execute_reply": "2024-12-05T05:19:02.112102Z",
     "shell.execute_reply.started": "2024-12-05T05:19:02.083124Z"
    }
   },
   "source": [
    "## Setup Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:46:31.529083Z",
     "iopub.status.busy": "2025-02-21T03:46:31.528747Z",
     "iopub.status.idle": "2025-02-21T03:46:32.715029Z",
     "shell.execute_reply": "2025-02-21T03:46:32.713762Z",
     "shell.execute_reply.started": "2025-02-21T03:46:31.529050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/kaggle/working/': Device or resource busy\n"
     ]
    }
   ],
   "source": [
    "!rm -r /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:46:32.717319Z",
     "iopub.status.busy": "2025-02-21T03:46:32.717035Z",
     "iopub.status.idle": "2025-02-21T03:46:34.774527Z",
     "shell.execute_reply": "2025-02-21T03:46:34.773142Z",
     "shell.execute_reply.started": "2025-02-21T03:46:32.717292Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir ./models\n",
    "!mkdir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:46:34.777482Z",
     "iopub.status.busy": "2025-02-21T03:46:34.777091Z",
     "iopub.status.idle": "2025-02-21T03:46:48.685956Z",
     "shell.execute_reply": "2025-02-21T03:46:48.684865Z",
     "shell.execute_reply.started": "2025-02-21T03:46:34.777441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools~=2.0.4\n",
      "  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools~=2.0.4) (3.7.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools~=2.0.4) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.4) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools~=2.0.4) (1.16.0)\n",
      "Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools~=2.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Declare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:46:48.688129Z",
     "iopub.status.busy": "2025-02-21T03:46:48.687837Z",
     "iopub.status.idle": "2025-02-21T03:46:48.692923Z",
     "shell.execute_reply": "2025-02-21T03:46:48.691982Z",
     "shell.execute_reply.started": "2025-02-21T03:46:48.688095Z"
    }
   },
   "outputs": [],
   "source": [
    "coco_dataset_path=\"/kaggle/input/coco-2017-dataset\"\n",
    "coco2017_train_img_path = \"coco2017/train2017/\"\n",
    "coco2017_test_img_path = \"coco2017/test2017/\"\n",
    "coco2017_val_img_path = \"coco2017/val2017/\"\n",
    "coco2017_train_annotation_path = \"coco2017/annotations/captions_train2017.json\"\n",
    "coco2017_test_annotation_path = \"coco2017/annotations/image_info_test2017.json\"\n",
    "coco2017_val_annotation_path = \"coco2017/annotations/captions_val2017.json\"\n",
    "\n",
    "vocab_file=\"/kaggle/working/data/vocab.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:46:48.695025Z",
     "iopub.status.busy": "2025-02-21T03:46:48.694291Z",
     "iopub.status.idle": "2025-02-21T03:46:48.715984Z",
     "shell.execute_reply": "2025-02-21T03:46:48.715393Z",
     "shell.execute_reply.started": "2025-02-21T03:46:48.694984Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "annotations_file = os.path.join(\n",
    "    coco_dataset_path, coco2017_train_annotation_path\n",
    ")\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_threshold,\n",
    "        vocab_file=vocab_file,\n",
    "        start_word=\"<start>\",\n",
    "        end_word=\"<end>\",\n",
    "        unk_word=\"<unk>\",\n",
    "        annotations_file=annotations_file,\n",
    "        vocab_from_file=False,\n",
    "    ):\n",
    "        self.vocab_threshold = vocab_threshold\n",
    "        self.vocab_file = vocab_file\n",
    "        self.start_word = start_word\n",
    "        self.end_word = end_word\n",
    "        self.unk_word = unk_word\n",
    "        self.annotations_file = annotations_file\n",
    "        self.vocab_from_file = vocab_from_file\n",
    "        self.get_vocab()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        if os.path.exists(self.vocab_file) and self.vocab_from_file:\n",
    "            with open(self.vocab_file, \"rb\") as f:\n",
    "                print(\"self.vocab_file : \", self.vocab_file)\n",
    "                print(\"file :\",f)\n",
    "                vocab = pickle.load(f)\n",
    "            self.word2idx = vocab.word2idx\n",
    "            self.idx2word = vocab.idx2word\n",
    "            print(\"Vocabulary successfully loaded from vocab.pkl file!\")\n",
    "\n",
    "        # create a new vocab file\n",
    "        else:\n",
    "            self.build_vocab()\n",
    "            with open(self.vocab_file, \"wb\") as f:\n",
    "                pickle.dump(self, f)\n",
    "\n",
    "    def build_vocab(self):\n",
    "        self.init_vocab()\n",
    "        self.add_word(self.start_word)\n",
    "        self.add_word(self.end_word)\n",
    "        self.add_word(self.unk_word)\n",
    "        self.add_captions()\n",
    "\n",
    "    def init_vocab(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def add_captions(self):\n",
    "        coco = COCO(self.annotations_file)\n",
    "        counter = Counter()\n",
    "        ids = coco.anns.keys()\n",
    "        for i, idx in enumerate(ids):\n",
    "            caption = str(coco.anns[idx][\"caption\"])\n",
    "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "            counter.update(tokens)\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
    "                \n",
    "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx[self.unk_word]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:46:48.718018Z",
     "iopub.status.busy": "2025-02-21T03:46:48.717765Z",
     "iopub.status.idle": "2025-02-21T03:46:55.698794Z",
     "shell.execute_reply": "2025-02-21T03:46:55.698106Z",
     "shell.execute_reply.started": "2025-02-21T03:46:48.717993Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_loader(\n",
    "    transform,\n",
    "    mode=\"train\",\n",
    "    batch_size=1,\n",
    "    vocab_threshold=None,\n",
    "    vocab_file=vocab_file,\n",
    "    start_word=\"<start>\",\n",
    "    end_word=\"<end>\",\n",
    "    unk_word=\"<unk>\",\n",
    "    vocab_from_file=True,\n",
    "    num_workers=0,\n",
    "    cocoapi_loc=coco_dataset_path,\n",
    "):\n",
    "    if mode == \"train\":\n",
    "        img_folder = os.path.join(cocoapi_loc, coco2017_train_img_path)\n",
    "        annotations_file = os.path.join(\n",
    "            cocoapi_loc, coco2017_train_annotation_path\n",
    "        )\n",
    "    elif mode == \"valid\":\n",
    "        img_folder = os.path.join(cocoapi_loc, coco2017_val_img_path)\n",
    "        annotations_file = os.path.join(\n",
    "            cocoapi_loc, coco2017_val_annotation_path\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}\")\n",
    "    # COCO caption dataset.\n",
    "    dataset = CoCoDataset(\n",
    "        transform=transform,\n",
    "        mode=mode,\n",
    "        batch_size=batch_size,\n",
    "        vocab_threshold=vocab_threshold,\n",
    "        vocab_file=vocab_file,\n",
    "        start_word=start_word,\n",
    "        end_word=end_word,\n",
    "        unk_word=unk_word,\n",
    "        annotations_file=annotations_file,\n",
    "        vocab_from_file=vocab_from_file,\n",
    "        img_folder=img_folder,\n",
    "    )\n",
    "\n",
    "    if mode == \"train\":\n",
    "        indices = dataset.get_train_indices()\n",
    "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        # data loader for COCO dataset.\n",
    "        data_loader = data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_sampler=data.sampler.BatchSampler(\n",
    "                sampler=initial_sampler, batch_size=dataset.batch_size, drop_last=False\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        data_loader = data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=dataset.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "class CoCoDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform,\n",
    "        mode,\n",
    "        batch_size,\n",
    "        vocab_threshold,\n",
    "        vocab_file,\n",
    "        start_word,\n",
    "        end_word,\n",
    "        unk_word,\n",
    "        annotations_file,\n",
    "        vocab_from_file,\n",
    "        img_folder,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab = Vocabulary(\n",
    "            vocab_threshold,\n",
    "            vocab_file,\n",
    "            start_word,\n",
    "            end_word,\n",
    "            unk_word,\n",
    "            annotations_file,\n",
    "            vocab_from_file,\n",
    "        )\n",
    "        self.img_folder = img_folder\n",
    "        if self.mode == \"train\":\n",
    "            self.coco = COCO(annotations_file)\n",
    "            self.ids = list(self.coco.anns.keys())\n",
    "            print(\"Obtaining caption lengths...\")\n",
    "            all_tokens = [\n",
    "                nltk.tokenize.word_tokenize(\n",
    "                    str(self.coco.anns[self.ids[index]][\"caption\"]).lower()\n",
    "                )\n",
    "                for index in tqdm(np.arange(len(self.ids)))\n",
    "            ]\n",
    "            self.caption_lengths = [len(token) for token in all_tokens]\n",
    "        else:\n",
    "            test_info = json.loads(open(annotations_file).read())\n",
    "            self.paths = [item[\"file_name\"] for item in test_info[\"images\"]]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # obtain image and caption if in training mode\n",
    "        if self.mode == \"train\":\n",
    "            ann_id = self.ids[index]\n",
    "            caption = self.coco.anns[ann_id][\"caption\"]\n",
    "            img_id = self.coco.anns[ann_id][\"image_id\"]\n",
    "            path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # Convert caption to tensor of word ids.\n",
    "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "            caption = []\n",
    "            caption.append(self.vocab(self.vocab.start_word))\n",
    "            caption.extend([self.vocab(token) for token in tokens])\n",
    "            caption.append(self.vocab(self.vocab.end_word))\n",
    "            caption = torch.Tensor(caption).long()\n",
    "\n",
    "            # return pre-processed image and caption tensors\n",
    "            return image, caption\n",
    "\n",
    "        elif self.mode == \"valid\":\n",
    "            path = self.paths[index]\n",
    "            image_id = int(path.split(\"/\")[0].split(\".\")[0].split(\"_\")[-1])\n",
    "            pil_image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            image = self.transform(pil_image)\n",
    "\n",
    "            # return original image and pre-processed image tensor\n",
    "            return image_id, image\n",
    "        # obtain image if in test mode\n",
    "        else:\n",
    "            path = self.paths[index]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            pil_image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            orig_image = np.array(pil_image)\n",
    "            image = self.transform(pil_image)\n",
    "\n",
    "            # return original image and pre-processed image tensor\n",
    "            return orig_image, image\n",
    "\n",
    "    def get_train_indices(self):\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "        all_indices = np.where(\n",
    "            [\n",
    "                self.caption_lengths[i] == sel_length\n",
    "                for i in np.arange(len(self.caption_lengths))\n",
    "            ]\n",
    "        )[0]\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == \"train\":\n",
    "            return len(self.ids)\n",
    "        else:\n",
    "            return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:46:55.700209Z",
     "iopub.status.busy": "2025-02-21T03:46:55.699822Z",
     "iopub.status.idle": "2025-02-21T03:46:57.680989Z",
     "shell.execute_reply": "2025-02-21T03:46:57.680064Z",
     "shell.execute_reply.started": "2025-02-21T03:46:55.700181Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:46:57.683768Z",
     "iopub.status.busy": "2025-02-21T03:46:57.682877Z",
     "iopub.status.idle": "2025-02-21T03:46:57.716291Z",
     "shell.execute_reply": "2025-02-21T03:46:57.715473Z",
     "shell.execute_reply.started": "2025-02-21T03:46:57.683720Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # batch size\n",
    "vocab_threshold = 2  # minimum word count threshold\n",
    "vocab_from_file = False  # if True, load existing vocab file\n",
    "embed_size = 512  # dimensionality of image and word embeddings\n",
    "hidden_size = 1024  # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 5  # number of training epochs\n",
    "save_every = 1  # determines frequency of saving model weights\n",
    "print_every = 20  # determines window for printing average loss\n",
    "log_file = \"training_log.txt\"  # name of file with saved training loss and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:46:57.717590Z",
     "iopub.status.busy": "2025-02-21T03:46:57.717327Z",
     "iopub.status.idle": "2025-02-21T03:46:57.750470Z",
     "shell.execute_reply": "2025-02-21T03:46:57.749670Z",
     "shell.execute_reply.started": "2025-02-21T03:46:57.717565Z"
    }
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:46:57.751950Z",
     "iopub.status.busy": "2025-02-21T03:46:57.751593Z",
     "iopub.status.idle": "2025-02-21T03:48:51.066801Z",
     "shell.execute_reply": "2025-02-21T03:48:51.066068Z",
     "shell.execute_reply.started": "2025-02-21T03:46:57.751914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.35s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/591753] Tokenizing captions...\n",
      "[100000/591753] Tokenizing captions...\n",
      "[200000/591753] Tokenizing captions...\n",
      "[300000/591753] Tokenizing captions...\n",
      "[400000/591753] Tokenizing captions...\n",
      "[500000/591753] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=1.07s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036f414ade1149b8b27502d8f6a54f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591753 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build data loader.\n",
    "data_loader = get_loader(\n",
    "    transform=transform_train,\n",
    "    mode=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_from_file=vocab_from_file,\n",
    "    cocoapi_loc=coco_dataset_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:48:53.329738Z",
     "iopub.status.busy": "2025-02-21T03:48:53.329439Z",
     "iopub.status.idle": "2025-02-21T03:48:53.372315Z",
     "shell.execute_reply": "2025-02-21T03:48:53.371613Z",
     "shell.execute_reply.started": "2025-02-21T03:48:53.329682Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# ----------- Encoder ------------\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # disable learning for parameters\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "# --------- Decoder ----------\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Assigning hidden dimension\n",
    "        self.hidden_dim = hidden_size\n",
    "        # Map each word index to a dense word embedding tensor of embed_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        # Creating LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Initializing linear to apply at last of RNN layer for further prediction\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # Initializing values for hidden and cell state\n",
    "        self.hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # remove <end> token from captions and embed captions\n",
    "        cap_embedding = self.embed(\n",
    "            captions[:, :-1]\n",
    "        )  # (bs, cap_length) -> (bs, cap_length-1, embed_size)\n",
    "\n",
    "        embeddings = torch.cat((features.unsqueeze(dim=1), cap_embedding), dim=1)\n",
    "\n",
    "        #  getting output i.e. score and hidden layer.\n",
    "        # first value: all the hidden states throughout the sequence. second value: the most recent hidden state\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeddings\n",
    "        )  # (bs, cap_length, hidden_size), (1, bs, hidden_size)\n",
    "        outputs = self.linear(lstm_out)  # (bs, cap_length, vocab_size)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        res = []\n",
    "\n",
    "        # Now we feed the LSTM output and hidden states back into itself to get the caption\n",
    "        for i in range(max_len):\n",
    "            lstm_out, states = self.lstm(\n",
    "                inputs, states\n",
    "            )  # lstm_out: (1, 1, hidden_size)\n",
    "            outputs = self.linear(lstm_out.squeeze(dim=1))  # outputs: (1, vocab_size)\n",
    "            _, predicted_idx = outputs.max(dim=1)  # predicted: (1, 1)\n",
    "            res.append(predicted_idx.item())\n",
    "            # if the predicted idx is the stop index, the loop stops\n",
    "            if predicted_idx == 1:\n",
    "                break\n",
    "            inputs = self.embed(predicted_idx)  # inputs: (1, embed_size)\n",
    "            # prepare input for next iteration\n",
    "            inputs = inputs.unsqueeze(1)  # inputs: (1, 1, embed_size)\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Step Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:48:53.373665Z",
     "iopub.status.busy": "2025-02-21T03:48:53.373422Z",
     "iopub.status.idle": "2025-02-21T03:48:54.198471Z",
     "shell.execute_reply": "2025-02-21T03:48:54.197633Z",
     "shell.execute_reply.started": "2025-02-21T03:48:53.373640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is :  16807\n"
     ]
    }
   ],
   "source": [
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(\"vocab size is : \",vocab_size)\n",
    "\n",
    "# Initializing the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Defining the loss function\n",
    "criterion = (\n",
    "    nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    ")\n",
    "\n",
    "# Specifying the learnable parameters of the mode\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Defining the optimize\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoc\n",
    "total_step = math.ceil(len(data_loader.dataset) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:48:54.199962Z",
     "iopub.status.busy": "2025-02-21T03:48:54.199593Z",
     "iopub.status.idle": "2025-02-21T03:48:54.238253Z",
     "shell.execute_reply": "2025-02-21T03:48:54.237432Z",
     "shell.execute_reply.started": "2025-02-21T03:48:54.199933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Device Name:  Tesla T4\n",
      "Memory Usage:\n",
      "Allocated: 0.2 GB\n",
      "Cached:    0.4 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(\"Device Name: \", torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T03:49:14.247741Z",
     "iopub.status.busy": "2025-02-21T03:49:14.247056Z",
     "iopub.status.idle": "2025-02-21T13:05:36.629715Z",
     "shell.execute_reply": "2025-02-21T13:05:36.628815Z",
     "shell.execute_reply.started": "2025-02-21T03:49:14.247704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [20/4624], Loss: 4.6251, Perplexity: 102.0138\n",
      "Epoch [1/3], Step [40/4624], Loss: 4.0229, Perplexity: 55.8619\n",
      "Epoch [1/3], Step [60/4624], Loss: 3.7007, Perplexity: 40.4755\n",
      "Epoch [1/3], Step [80/4624], Loss: 3.5056, Perplexity: 33.3015\n",
      "Epoch [1/3], Step [100/4624], Loss: 3.4473, Perplexity: 31.4160\n",
      "Epoch [1/3], Step [120/4624], Loss: 3.6320, Perplexity: 37.7897\n",
      "Epoch [1/3], Step [140/4624], Loss: 3.4331, Perplexity: 30.9723\n",
      "Epoch [1/3], Step [160/4624], Loss: 3.9719, Perplexity: 53.0873\n",
      "Epoch [1/3], Step [180/4624], Loss: 3.4373, Perplexity: 31.1022\n",
      "Epoch [1/3], Step [200/4624], Loss: 3.1775, Perplexity: 23.9868\n",
      "Epoch [1/3], Step [220/4624], Loss: 3.2655, Perplexity: 26.1936\n",
      "Epoch [1/3], Step [240/4624], Loss: 3.1634, Perplexity: 23.6509\n",
      "Epoch [1/3], Step [260/4624], Loss: 3.0655, Perplexity: 21.4451\n",
      "Epoch [1/3], Step [280/4624], Loss: 3.1224, Perplexity: 22.7003\n",
      "Epoch [1/3], Step [300/4624], Loss: 3.1645, Perplexity: 23.6770\n",
      "Epoch [1/3], Step [320/4624], Loss: 3.0418, Perplexity: 20.9433\n",
      "Epoch [1/3], Step [340/4624], Loss: 3.5921, Perplexity: 36.3117\n",
      "Epoch [1/3], Step [360/4624], Loss: 2.9722, Perplexity: 19.5345\n",
      "Epoch [1/3], Step [380/4624], Loss: 2.9761, Perplexity: 19.6119\n",
      "Epoch [1/3], Step [400/4624], Loss: 2.7710, Perplexity: 15.9753\n",
      "Epoch [1/3], Step [420/4624], Loss: 3.2680, Perplexity: 26.2598\n",
      "Epoch [1/3], Step [440/4624], Loss: 2.9468, Perplexity: 19.0459\n",
      "Epoch [1/3], Step [460/4624], Loss: 2.8283, Perplexity: 16.9169\n",
      "Epoch [1/3], Step [480/4624], Loss: 2.6298, Perplexity: 13.8715\n",
      "Epoch [1/3], Step [500/4624], Loss: 3.0261, Perplexity: 20.6170\n",
      "Epoch [1/3], Step [520/4624], Loss: 2.9731, Perplexity: 19.5531\n",
      "Epoch [1/3], Step [540/4624], Loss: 2.7382, Perplexity: 15.4590\n",
      "Epoch [1/3], Step [560/4624], Loss: 2.7707, Perplexity: 15.9701\n",
      "Epoch [1/3], Step [580/4624], Loss: 2.9616, Perplexity: 19.3291\n",
      "Epoch [1/3], Step [600/4624], Loss: 2.7521, Perplexity: 15.6755\n",
      "Epoch [1/3], Step [620/4624], Loss: 2.8215, Perplexity: 16.8022\n",
      "Epoch [1/3], Step [640/4624], Loss: 2.6586, Perplexity: 14.2757\n",
      "Epoch [1/3], Step [660/4624], Loss: 2.6765, Perplexity: 14.5348\n",
      "Epoch [1/3], Step [680/4624], Loss: 2.8476, Perplexity: 17.2464\n",
      "Epoch [1/3], Step [700/4624], Loss: 2.7121, Perplexity: 15.0609\n",
      "Epoch [1/3], Step [720/4624], Loss: 2.8425, Perplexity: 17.1589\n",
      "Epoch [1/3], Step [740/4624], Loss: 2.5690, Perplexity: 13.0524\n",
      "Epoch [1/3], Step [760/4624], Loss: 2.3941, Perplexity: 10.9582\n",
      "Epoch [1/3], Step [780/4624], Loss: 2.8030, Perplexity: 16.4937\n",
      "Epoch [1/3], Step [800/4624], Loss: 2.4033, Perplexity: 11.0593\n",
      "Epoch [1/3], Step [820/4624], Loss: 2.4287, Perplexity: 11.3436\n",
      "Epoch [1/3], Step [840/4624], Loss: 2.5023, Perplexity: 12.2108\n",
      "Epoch [1/3], Step [860/4624], Loss: 2.9862, Perplexity: 19.8106\n",
      "Epoch [1/3], Step [880/4624], Loss: 2.7089, Perplexity: 15.0133\n",
      "Epoch [1/3], Step [900/4624], Loss: 2.4574, Perplexity: 11.6747\n",
      "Epoch [1/3], Step [920/4624], Loss: 2.5609, Perplexity: 12.9469\n",
      "Epoch [1/3], Step [940/4624], Loss: 2.5312, Perplexity: 12.5684\n",
      "Epoch [1/3], Step [960/4624], Loss: 2.4976, Perplexity: 12.1530\n",
      "Epoch [1/3], Step [980/4624], Loss: 2.6652, Perplexity: 14.3706\n",
      "Epoch [1/3], Step [1000/4624], Loss: 2.6542, Perplexity: 14.2142\n",
      "Epoch [1/3], Step [1020/4624], Loss: 2.8305, Perplexity: 16.9542\n",
      "Epoch [1/3], Step [1040/4624], Loss: 2.4824, Perplexity: 11.9698\n",
      "Epoch [1/3], Step [1060/4624], Loss: 2.4800, Perplexity: 11.9408\n",
      "Epoch [1/3], Step [1080/4624], Loss: 2.3559, Perplexity: 10.5480\n",
      "Epoch [1/3], Step [1100/4624], Loss: 3.2260, Perplexity: 25.1777\n",
      "Epoch [1/3], Step [1120/4624], Loss: 2.4437, Perplexity: 11.5159\n",
      "Epoch [1/3], Step [1140/4624], Loss: 2.4560, Perplexity: 11.6583\n",
      "Epoch [1/3], Step [1160/4624], Loss: 2.4861, Perplexity: 12.0144\n",
      "Epoch [1/3], Step [1180/4624], Loss: 2.3305, Perplexity: 10.2827\n",
      "Epoch [1/3], Step [1200/4624], Loss: 2.7783, Perplexity: 16.0922\n",
      "Epoch [1/3], Step [1220/4624], Loss: 2.3013, Perplexity: 9.9870\n",
      "Epoch [1/3], Step [1240/4624], Loss: 2.2739, Perplexity: 9.7169\n",
      "Epoch [1/3], Step [1260/4624], Loss: 2.3357, Perplexity: 10.3363\n",
      "Epoch [1/3], Step [1280/4624], Loss: 2.3147, Perplexity: 10.1220\n",
      "Epoch [1/3], Step [1300/4624], Loss: 2.2666, Perplexity: 9.6464\n",
      "Epoch [1/3], Step [1320/4624], Loss: 2.4469, Perplexity: 11.5527\n",
      "Epoch [1/3], Step [1340/4624], Loss: 2.3574, Perplexity: 10.5631\n",
      "Epoch [1/3], Step [1360/4624], Loss: 2.3329, Perplexity: 10.3082\n",
      "Epoch [1/3], Step [1380/4624], Loss: 2.1832, Perplexity: 8.8743\n",
      "Epoch [1/3], Step [1400/4624], Loss: 2.6411, Perplexity: 14.0292\n",
      "Epoch [1/3], Step [1420/4624], Loss: 2.2132, Perplexity: 9.1448\n",
      "Epoch [1/3], Step [1440/4624], Loss: 2.2672, Perplexity: 9.6525\n",
      "Epoch [1/3], Step [1460/4624], Loss: 2.1488, Perplexity: 8.5745\n",
      "Epoch [1/3], Step [1480/4624], Loss: 2.2125, Perplexity: 9.1384\n",
      "Epoch [1/3], Step [1500/4624], Loss: 2.3971, Perplexity: 10.9916\n",
      "Epoch [1/3], Step [1520/4624], Loss: 2.4841, Perplexity: 11.9899\n",
      "Epoch [1/3], Step [1540/4624], Loss: 2.2486, Perplexity: 9.4741\n",
      "Epoch [1/3], Step [1560/4624], Loss: 2.7301, Perplexity: 15.3348\n",
      "Epoch [1/3], Step [1580/4624], Loss: 2.3572, Perplexity: 10.5610\n",
      "Epoch [1/3], Step [1600/4624], Loss: 2.4358, Perplexity: 11.4252\n",
      "Epoch [1/3], Step [1620/4624], Loss: 2.2999, Perplexity: 9.9735\n",
      "Epoch [1/3], Step [1640/4624], Loss: 2.1309, Perplexity: 8.4226\n",
      "Epoch [1/3], Step [1660/4624], Loss: 2.3152, Perplexity: 10.1266\n",
      "Epoch [1/3], Step [1680/4624], Loss: 2.4083, Perplexity: 11.1156\n",
      "Epoch [1/3], Step [1700/4624], Loss: 2.4747, Perplexity: 11.8779\n",
      "Epoch [1/3], Step [1720/4624], Loss: 2.4390, Perplexity: 11.4621\n",
      "Epoch [1/3], Step [1740/4624], Loss: 2.3196, Perplexity: 10.1712\n",
      "Epoch [1/3], Step [1760/4624], Loss: 2.2908, Perplexity: 9.8831\n",
      "Epoch [1/3], Step [1780/4624], Loss: 2.6186, Perplexity: 13.7164\n",
      "Epoch [1/3], Step [1800/4624], Loss: 2.1049, Perplexity: 8.2061\n",
      "Epoch [1/3], Step [1820/4624], Loss: 2.3843, Perplexity: 10.8510\n",
      "Epoch [1/3], Step [1840/4624], Loss: 2.3575, Perplexity: 10.5647\n",
      "Epoch [1/3], Step [1860/4624], Loss: 2.0867, Perplexity: 8.0579\n",
      "Epoch [1/3], Step [1880/4624], Loss: 2.3536, Perplexity: 10.5238\n",
      "Epoch [1/3], Step [1900/4624], Loss: 2.3067, Perplexity: 10.0411\n",
      "Epoch [1/3], Step [1920/4624], Loss: 2.0487, Perplexity: 7.7581\n",
      "Epoch [1/3], Step [1940/4624], Loss: 2.2831, Perplexity: 9.8072\n",
      "Epoch [1/3], Step [1960/4624], Loss: 2.7696, Perplexity: 15.9527\n",
      "Epoch [1/3], Step [1980/4624], Loss: 2.2757, Perplexity: 9.7352\n",
      "Epoch [1/3], Step [2000/4624], Loss: 2.2199, Perplexity: 9.2062\n",
      "Epoch [1/3], Step [2020/4624], Loss: 2.3350, Perplexity: 10.3295\n",
      "Epoch [1/3], Step [2040/4624], Loss: 2.1495, Perplexity: 8.5807\n",
      "Epoch [1/3], Step [2060/4624], Loss: 2.3830, Perplexity: 10.8374\n",
      "Epoch [1/3], Step [2080/4624], Loss: 2.1773, Perplexity: 8.8224\n",
      "Epoch [1/3], Step [2100/4624], Loss: 2.2292, Perplexity: 9.2924\n",
      "Epoch [1/3], Step [2120/4624], Loss: 2.3660, Perplexity: 10.6544\n",
      "Epoch [1/3], Step [2140/4624], Loss: 2.3986, Perplexity: 11.0080\n",
      "Epoch [1/3], Step [2160/4624], Loss: 2.0925, Perplexity: 8.1054\n",
      "Epoch [1/3], Step [2180/4624], Loss: 2.2699, Perplexity: 9.6783\n",
      "Epoch [1/3], Step [2200/4624], Loss: 2.2397, Perplexity: 9.3907\n",
      "Epoch [1/3], Step [2220/4624], Loss: 2.1783, Perplexity: 8.8317\n",
      "Epoch [1/3], Step [2240/4624], Loss: 2.2967, Perplexity: 9.9415\n",
      "Epoch [1/3], Step [2260/4624], Loss: 2.1684, Perplexity: 8.7446\n",
      "Epoch [1/3], Step [2280/4624], Loss: 2.1146, Perplexity: 8.2863\n",
      "Epoch [1/3], Step [2300/4624], Loss: 2.0439, Perplexity: 7.7210\n",
      "Epoch [1/3], Step [2320/4624], Loss: 2.3772, Perplexity: 10.7743\n",
      "Epoch [1/3], Step [2340/4624], Loss: 2.2357, Perplexity: 9.3528\n",
      "Epoch [1/3], Step [2360/4624], Loss: 2.1422, Perplexity: 8.5181\n",
      "Epoch [1/3], Step [2380/4624], Loss: 2.4949, Perplexity: 12.1201\n",
      "Epoch [1/3], Step [2400/4624], Loss: 2.6933, Perplexity: 14.7802\n",
      "Epoch [1/3], Step [2420/4624], Loss: 2.2972, Perplexity: 9.9467\n",
      "Epoch [1/3], Step [2440/4624], Loss: 2.1804, Perplexity: 8.8500\n",
      "Epoch [1/3], Step [2460/4624], Loss: 2.1164, Perplexity: 8.3015\n",
      "Epoch [1/3], Step [2480/4624], Loss: 2.0503, Perplexity: 7.7704\n",
      "Epoch [1/3], Step [2500/4624], Loss: 2.8878, Perplexity: 17.9529\n",
      "Epoch [1/3], Step [2520/4624], Loss: 2.0702, Perplexity: 7.9260\n",
      "Epoch [1/3], Step [2540/4624], Loss: 2.2142, Perplexity: 9.1541\n",
      "Epoch [1/3], Step [2560/4624], Loss: 2.4792, Perplexity: 11.9316\n",
      "Epoch [1/3], Step [2580/4624], Loss: 2.0715, Perplexity: 7.9364\n",
      "Epoch [1/3], Step [2600/4624], Loss: 2.0810, Perplexity: 8.0129\n",
      "Epoch [1/3], Step [2620/4624], Loss: 2.7391, Perplexity: 15.4736\n",
      "Epoch [1/3], Step [2640/4624], Loss: 2.1488, Perplexity: 8.5742\n",
      "Epoch [1/3], Step [2660/4624], Loss: 2.0863, Perplexity: 8.0550\n",
      "Epoch [1/3], Step [2680/4624], Loss: 3.0511, Perplexity: 21.1389\n",
      "Epoch [1/3], Step [2700/4624], Loss: 2.1672, Perplexity: 8.7338\n",
      "Epoch [1/3], Step [2720/4624], Loss: 2.3238, Perplexity: 10.2142\n",
      "Epoch [1/3], Step [2740/4624], Loss: 2.1007, Perplexity: 8.1719\n",
      "Epoch [1/3], Step [2760/4624], Loss: 2.3585, Perplexity: 10.5755\n",
      "Epoch [1/3], Step [2780/4624], Loss: 2.0913, Perplexity: 8.0953\n",
      "Epoch [1/3], Step [2800/4624], Loss: 2.1436, Perplexity: 8.5299\n",
      "Epoch [1/3], Step [2820/4624], Loss: 2.3063, Perplexity: 10.0375\n",
      "Epoch [1/3], Step [2840/4624], Loss: 2.5125, Perplexity: 12.3357\n",
      "Epoch [1/3], Step [2860/4624], Loss: 1.9948, Perplexity: 7.3504\n",
      "Epoch [1/3], Step [2880/4624], Loss: 2.1328, Perplexity: 8.4385\n",
      "Epoch [1/3], Step [2900/4624], Loss: 2.0504, Perplexity: 7.7711\n",
      "Epoch [1/3], Step [2920/4624], Loss: 2.1431, Perplexity: 8.5255\n",
      "Epoch [1/3], Step [2940/4624], Loss: 2.2949, Perplexity: 9.9234\n",
      "Epoch [1/3], Step [2960/4624], Loss: 2.1343, Perplexity: 8.4510\n",
      "Epoch [1/3], Step [2980/4624], Loss: 2.0911, Perplexity: 8.0934\n",
      "Epoch [1/3], Step [3000/4624], Loss: 2.2662, Perplexity: 9.6424\n",
      "Epoch [1/3], Step [3020/4624], Loss: 2.1681, Perplexity: 8.7420\n",
      "Epoch [1/3], Step [3040/4624], Loss: 2.0588, Perplexity: 7.8364\n",
      "Epoch [1/3], Step [3060/4624], Loss: 2.2188, Perplexity: 9.1960\n",
      "Epoch [1/3], Step [3080/4624], Loss: 2.2314, Perplexity: 9.3132\n",
      "Epoch [1/3], Step [3100/4624], Loss: 2.0041, Perplexity: 7.4195\n",
      "Epoch [1/3], Step [3120/4624], Loss: 2.0744, Perplexity: 7.9600\n",
      "Epoch [1/3], Step [3140/4624], Loss: 2.3424, Perplexity: 10.4065\n",
      "Epoch [1/3], Step [3160/4624], Loss: 2.1445, Perplexity: 8.5375\n",
      "Epoch [1/3], Step [3180/4624], Loss: 2.0772, Perplexity: 7.9825\n",
      "Epoch [1/3], Step [3200/4624], Loss: 2.0779, Perplexity: 7.9874\n",
      "Epoch [1/3], Step [3220/4624], Loss: 2.1178, Perplexity: 8.3128\n",
      "Epoch [1/3], Step [3240/4624], Loss: 2.2961, Perplexity: 9.9353\n",
      "Epoch [1/3], Step [3260/4624], Loss: 2.1091, Perplexity: 8.2410\n",
      "Epoch [1/3], Step [3280/4624], Loss: 2.0051, Perplexity: 7.4272\n",
      "Epoch [1/3], Step [3300/4624], Loss: 2.4014, Perplexity: 11.0381\n",
      "Epoch [1/3], Step [3320/4624], Loss: 2.1563, Perplexity: 8.6395\n",
      "Epoch [1/3], Step [3340/4624], Loss: 2.0677, Perplexity: 7.9065\n",
      "Epoch [1/3], Step [3360/4624], Loss: 2.0632, Perplexity: 7.8712\n",
      "Epoch [1/3], Step [3380/4624], Loss: 1.9590, Perplexity: 7.0919\n",
      "Epoch [1/3], Step [3400/4624], Loss: 3.1614, Perplexity: 23.6045\n",
      "Epoch [1/3], Step [3420/4624], Loss: 2.0907, Perplexity: 8.0906\n",
      "Epoch [1/3], Step [3440/4624], Loss: 2.1061, Perplexity: 8.2163\n",
      "Epoch [1/3], Step [3460/4624], Loss: 2.0956, Perplexity: 8.1303\n",
      "Epoch [1/3], Step [3480/4624], Loss: 2.5550, Perplexity: 12.8719\n",
      "Epoch [1/3], Step [3500/4624], Loss: 2.2510, Perplexity: 9.4974\n",
      "Epoch [1/3], Step [3520/4624], Loss: 2.0146, Perplexity: 7.4979\n",
      "Epoch [1/3], Step [3540/4624], Loss: 2.2843, Perplexity: 9.8184\n",
      "Epoch [1/3], Step [3560/4624], Loss: 2.0906, Perplexity: 8.0899\n",
      "Epoch [1/3], Step [3580/4624], Loss: 1.9468, Perplexity: 7.0062\n",
      "Epoch [1/3], Step [3600/4624], Loss: 2.0681, Perplexity: 7.9101\n",
      "Epoch [1/3], Step [3620/4624], Loss: 1.9946, Perplexity: 7.3495\n",
      "Epoch [1/3], Step [3640/4624], Loss: 2.2719, Perplexity: 9.6982\n",
      "Epoch [1/3], Step [3660/4624], Loss: 2.1353, Perplexity: 8.4595\n",
      "Epoch [1/3], Step [3680/4624], Loss: 2.0500, Perplexity: 7.7682\n",
      "Epoch [1/3], Step [3700/4624], Loss: 2.1910, Perplexity: 8.9446\n",
      "Epoch [1/3], Step [3720/4624], Loss: 2.2290, Perplexity: 9.2905\n",
      "Epoch [1/3], Step [3740/4624], Loss: 2.0638, Perplexity: 7.8761\n",
      "Epoch [1/3], Step [3760/4624], Loss: 2.0197, Perplexity: 7.5362\n",
      "Epoch [1/3], Step [3780/4624], Loss: 1.9482, Perplexity: 7.0163\n",
      "Epoch [1/3], Step [3800/4624], Loss: 1.9901, Perplexity: 7.3161\n",
      "Epoch [1/3], Step [3820/4624], Loss: 1.9966, Perplexity: 7.3640\n",
      "Epoch [1/3], Step [3840/4624], Loss: 1.9466, Perplexity: 7.0050\n",
      "Epoch [1/3], Step [3860/4624], Loss: 1.9571, Perplexity: 7.0786\n",
      "Epoch [1/3], Step [3880/4624], Loss: 2.2762, Perplexity: 9.7397\n",
      "Epoch [1/3], Step [3900/4624], Loss: 1.9279, Perplexity: 6.8749\n",
      "Epoch [1/3], Step [3920/4624], Loss: 2.2808, Perplexity: 9.7844\n",
      "Epoch [1/3], Step [3940/4624], Loss: 2.2543, Perplexity: 9.5284\n",
      "Epoch [1/3], Step [3960/4624], Loss: 2.0057, Perplexity: 7.4312\n",
      "Epoch [1/3], Step [3980/4624], Loss: 2.1997, Perplexity: 9.0221\n",
      "Epoch [1/3], Step [4000/4624], Loss: 2.1451, Perplexity: 8.5426\n",
      "Epoch [1/3], Step [4020/4624], Loss: 2.0437, Perplexity: 7.7193\n",
      "Epoch [1/3], Step [4040/4624], Loss: 2.8028, Perplexity: 16.4910\n",
      "Epoch [1/3], Step [4060/4624], Loss: 2.1048, Perplexity: 8.2052\n",
      "Epoch [1/3], Step [4080/4624], Loss: 2.0295, Perplexity: 7.6102\n",
      "Epoch [1/3], Step [4100/4624], Loss: 1.9551, Perplexity: 7.0644\n",
      "Epoch [1/3], Step [4120/4624], Loss: 1.9194, Perplexity: 6.8171\n",
      "Epoch [1/3], Step [4140/4624], Loss: 2.2276, Perplexity: 9.2778\n",
      "Epoch [1/3], Step [4160/4624], Loss: 1.9241, Perplexity: 6.8492\n",
      "Epoch [1/3], Step [4180/4624], Loss: 1.9797, Perplexity: 7.2403\n",
      "Epoch [1/3], Step [4200/4624], Loss: 4.5065, Perplexity: 90.6043\n",
      "Epoch [1/3], Step [4220/4624], Loss: 1.9565, Perplexity: 7.0748\n",
      "Epoch [1/3], Step [4240/4624], Loss: 2.1104, Perplexity: 8.2512\n",
      "Epoch [1/3], Step [4260/4624], Loss: 2.2839, Perplexity: 9.8152\n",
      "Epoch [1/3], Step [4280/4624], Loss: 2.1618, Perplexity: 8.6865\n",
      "Epoch [1/3], Step [4300/4624], Loss: 2.1372, Perplexity: 8.4755\n",
      "Epoch [1/3], Step [4320/4624], Loss: 2.0487, Perplexity: 7.7576\n",
      "Epoch [1/3], Step [4340/4624], Loss: 2.0271, Perplexity: 7.5922\n",
      "Epoch [1/3], Step [4360/4624], Loss: 2.6369, Perplexity: 13.9693\n",
      "Epoch [1/3], Step [4380/4624], Loss: 2.0482, Perplexity: 7.7540\n",
      "Epoch [1/3], Step [4400/4624], Loss: 1.9559, Perplexity: 7.0705\n",
      "Epoch [1/3], Step [4420/4624], Loss: 2.0181, Perplexity: 7.5244\n",
      "Epoch [1/3], Step [4440/4624], Loss: 2.0088, Perplexity: 7.4545\n",
      "Epoch [1/3], Step [4460/4624], Loss: 2.1412, Perplexity: 8.5096\n",
      "Epoch [1/3], Step [4480/4624], Loss: 2.2510, Perplexity: 9.4976\n",
      "Epoch [1/3], Step [4500/4624], Loss: 2.1799, Perplexity: 8.8452\n",
      "Epoch [1/3], Step [4520/4624], Loss: 2.0012, Perplexity: 7.3977\n",
      "Epoch [1/3], Step [4540/4624], Loss: 1.9414, Perplexity: 6.9683\n",
      "Epoch [1/3], Step [4560/4624], Loss: 2.0131, Perplexity: 7.4865\n",
      "Epoch [1/3], Step [4580/4624], Loss: 2.0620, Perplexity: 7.8619\n",
      "Epoch [1/3], Step [4600/4624], Loss: 2.0809, Perplexity: 8.0118\n",
      "Epoch [1/3], Step [4620/4624], Loss: 2.1324, Perplexity: 8.4352\n",
      "Epoch [2/3], Step [20/4624], Loss: 2.1975, Perplexity: 9.0020\n",
      "Epoch [2/3], Step [40/4624], Loss: 2.1253, Perplexity: 8.3755\n",
      "Epoch [2/3], Step [60/4624], Loss: 2.0646, Perplexity: 7.8823\n",
      "Epoch [2/3], Step [80/4624], Loss: 2.1348, Perplexity: 8.4549\n",
      "Epoch [2/3], Step [100/4624], Loss: 1.9619, Perplexity: 7.1126\n",
      "Epoch [2/3], Step [120/4624], Loss: 2.1344, Perplexity: 8.4516\n",
      "Epoch [2/3], Step [140/4624], Loss: 2.4809, Perplexity: 11.9526\n",
      "Epoch [2/3], Step [160/4624], Loss: 1.9485, Perplexity: 7.0184\n",
      "Epoch [2/3], Step [180/4624], Loss: 2.7253, Perplexity: 15.2610\n",
      "Epoch [2/3], Step [200/4624], Loss: 2.0007, Perplexity: 7.3942\n",
      "Epoch [2/3], Step [220/4624], Loss: 1.8875, Perplexity: 6.6030\n",
      "Epoch [2/3], Step [240/4624], Loss: 1.9427, Perplexity: 6.9774\n",
      "Epoch [2/3], Step [260/4624], Loss: 1.9847, Perplexity: 7.2766\n",
      "Epoch [2/3], Step [280/4624], Loss: 1.9768, Perplexity: 7.2194\n",
      "Epoch [2/3], Step [300/4624], Loss: 1.9132, Perplexity: 6.7745\n",
      "Epoch [2/3], Step [320/4624], Loss: 1.8811, Perplexity: 6.5609\n",
      "Epoch [2/3], Step [340/4624], Loss: 2.0866, Perplexity: 8.0578\n",
      "Epoch [2/3], Step [360/4624], Loss: 1.9661, Perplexity: 7.1424\n",
      "Epoch [2/3], Step [380/4624], Loss: 2.0477, Perplexity: 7.7498\n",
      "Epoch [2/3], Step [400/4624], Loss: 2.4309, Perplexity: 11.3688\n",
      "Epoch [2/3], Step [420/4624], Loss: 2.1449, Perplexity: 8.5416\n",
      "Epoch [2/3], Step [440/4624], Loss: 1.9625, Perplexity: 7.1174\n",
      "Epoch [2/3], Step [460/4624], Loss: 2.0982, Perplexity: 8.1511\n",
      "Epoch [2/3], Step [480/4624], Loss: 1.9435, Perplexity: 6.9831\n",
      "Epoch [2/3], Step [500/4624], Loss: 2.0338, Perplexity: 7.6432\n",
      "Epoch [2/3], Step [520/4624], Loss: 2.0848, Perplexity: 8.0426\n",
      "Epoch [2/3], Step [540/4624], Loss: 1.9740, Perplexity: 7.1992\n",
      "Epoch [2/3], Step [560/4624], Loss: 2.0606, Perplexity: 7.8509\n",
      "Epoch [2/3], Step [580/4624], Loss: 2.0025, Perplexity: 7.4075\n",
      "Epoch [2/3], Step [600/4624], Loss: 1.8038, Perplexity: 6.0727\n",
      "Epoch [2/3], Step [620/4624], Loss: 1.9823, Perplexity: 7.2597\n",
      "Epoch [2/3], Step [640/4624], Loss: 2.2088, Perplexity: 9.1045\n",
      "Epoch [2/3], Step [660/4624], Loss: 1.9385, Perplexity: 6.9482\n",
      "Epoch [2/3], Step [680/4624], Loss: 2.1141, Perplexity: 8.2820\n",
      "Epoch [2/3], Step [700/4624], Loss: 1.9592, Perplexity: 7.0939\n",
      "Epoch [2/3], Step [720/4624], Loss: 1.9602, Perplexity: 7.1005\n",
      "Epoch [2/3], Step [740/4624], Loss: 1.9053, Perplexity: 6.7212\n",
      "Epoch [2/3], Step [760/4624], Loss: 2.0742, Perplexity: 7.9578\n",
      "Epoch [2/3], Step [780/4624], Loss: 2.1741, Perplexity: 8.7940\n",
      "Epoch [2/3], Step [800/4624], Loss: 2.1827, Perplexity: 8.8700\n",
      "Epoch [2/3], Step [820/4624], Loss: 1.9656, Perplexity: 7.1390\n",
      "Epoch [2/3], Step [840/4624], Loss: 1.9386, Perplexity: 6.9493\n",
      "Epoch [2/3], Step [860/4624], Loss: 1.9102, Perplexity: 6.7546\n",
      "Epoch [2/3], Step [880/4624], Loss: 1.9536, Perplexity: 7.0541\n",
      "Epoch [2/3], Step [900/4624], Loss: 2.0176, Perplexity: 7.5202\n",
      "Epoch [2/3], Step [920/4624], Loss: 1.8914, Perplexity: 6.6288\n",
      "Epoch [2/3], Step [940/4624], Loss: 2.2521, Perplexity: 9.5072\n",
      "Epoch [2/3], Step [960/4624], Loss: 1.8574, Perplexity: 6.4068\n",
      "Epoch [2/3], Step [980/4624], Loss: 2.0397, Perplexity: 7.6883\n",
      "Epoch [2/3], Step [1000/4624], Loss: 2.0077, Perplexity: 7.4459\n",
      "Epoch [2/3], Step [1020/4624], Loss: 2.0474, Perplexity: 7.7479\n",
      "Epoch [2/3], Step [1040/4624], Loss: 2.1692, Perplexity: 8.7514\n",
      "Epoch [2/3], Step [1060/4624], Loss: 1.8195, Perplexity: 6.1691\n",
      "Epoch [2/3], Step [1080/4624], Loss: 1.8650, Perplexity: 6.4557\n",
      "Epoch [2/3], Step [1100/4624], Loss: 2.1344, Perplexity: 8.4522\n",
      "Epoch [2/3], Step [1120/4624], Loss: 2.0834, Perplexity: 8.0321\n",
      "Epoch [2/3], Step [1140/4624], Loss: 1.9396, Perplexity: 6.9559\n",
      "Epoch [2/3], Step [1160/4624], Loss: 1.9026, Perplexity: 6.7030\n",
      "Epoch [2/3], Step [1180/4624], Loss: 1.9096, Perplexity: 6.7506\n",
      "Epoch [2/3], Step [1200/4624], Loss: 2.0812, Perplexity: 8.0145\n",
      "Epoch [2/3], Step [1220/4624], Loss: 1.9857, Perplexity: 7.2844\n",
      "Epoch [2/3], Step [1240/4624], Loss: 2.0194, Perplexity: 7.5337\n",
      "Epoch [2/3], Step [1260/4624], Loss: 2.0287, Perplexity: 7.6046\n",
      "Epoch [2/3], Step [1280/4624], Loss: 1.9585, Perplexity: 7.0887\n",
      "Epoch [2/3], Step [1300/4624], Loss: 2.1425, Perplexity: 8.5208\n",
      "Epoch [2/3], Step [1320/4624], Loss: 1.9379, Perplexity: 6.9440\n",
      "Epoch [2/3], Step [1340/4624], Loss: 1.9825, Perplexity: 7.2605\n",
      "Epoch [2/3], Step [1360/4624], Loss: 1.9063, Perplexity: 6.7282\n",
      "Epoch [2/3], Step [1380/4624], Loss: 2.0098, Perplexity: 7.4620\n",
      "Epoch [2/3], Step [1400/4624], Loss: 1.8923, Perplexity: 6.6349\n",
      "Epoch [2/3], Step [1420/4624], Loss: 2.0246, Perplexity: 7.5734\n",
      "Epoch [2/3], Step [1440/4624], Loss: 2.0291, Perplexity: 7.6073\n",
      "Epoch [2/3], Step [1460/4624], Loss: 1.9864, Perplexity: 7.2892\n",
      "Epoch [2/3], Step [1480/4624], Loss: 1.9182, Perplexity: 6.8086\n",
      "Epoch [2/3], Step [1500/4624], Loss: 1.8519, Perplexity: 6.3719\n",
      "Epoch [2/3], Step [1520/4624], Loss: 2.0002, Perplexity: 7.3904\n",
      "Epoch [2/3], Step [1540/4624], Loss: 2.2693, Perplexity: 9.6727\n",
      "Epoch [2/3], Step [1560/4624], Loss: 2.1524, Perplexity: 8.6057\n",
      "Epoch [2/3], Step [1580/4624], Loss: 1.8345, Perplexity: 6.2622\n",
      "Epoch [2/3], Step [1600/4624], Loss: 1.9493, Perplexity: 7.0237\n",
      "Epoch [2/3], Step [1620/4624], Loss: 2.1009, Perplexity: 8.1732\n",
      "Epoch [2/3], Step [1640/4624], Loss: 1.9841, Perplexity: 7.2722\n",
      "Epoch [2/3], Step [1660/4624], Loss: 2.0913, Perplexity: 8.0953\n",
      "Epoch [2/3], Step [1680/4624], Loss: 1.8975, Perplexity: 6.6690\n",
      "Epoch [2/3], Step [1700/4624], Loss: 2.1545, Perplexity: 8.6232\n",
      "Epoch [2/3], Step [1720/4624], Loss: 1.9385, Perplexity: 6.9480\n",
      "Epoch [2/3], Step [1740/4624], Loss: 1.9702, Perplexity: 7.1718\n",
      "Epoch [2/3], Step [1760/4624], Loss: 1.7971, Perplexity: 6.0319\n",
      "Epoch [2/3], Step [1780/4624], Loss: 2.0011, Perplexity: 7.3970\n",
      "Epoch [2/3], Step [1800/4624], Loss: 2.0790, Perplexity: 7.9961\n",
      "Epoch [2/3], Step [1820/4624], Loss: 1.9806, Perplexity: 7.2469\n",
      "Epoch [2/3], Step [1840/4624], Loss: 1.9280, Perplexity: 6.8756\n",
      "Epoch [2/3], Step [1860/4624], Loss: 1.8461, Perplexity: 6.3354\n",
      "Epoch [2/3], Step [1880/4624], Loss: 1.9053, Perplexity: 6.7216\n",
      "Epoch [2/3], Step [1900/4624], Loss: 2.0131, Perplexity: 7.4862\n",
      "Epoch [2/3], Step [1920/4624], Loss: 1.9024, Perplexity: 6.7021\n",
      "Epoch [2/3], Step [1940/4624], Loss: 1.7694, Perplexity: 5.8673\n",
      "Epoch [2/3], Step [1960/4624], Loss: 2.0657, Perplexity: 7.8905\n",
      "Epoch [2/3], Step [1980/4624], Loss: 1.7736, Perplexity: 5.8920\n",
      "Epoch [2/3], Step [2000/4624], Loss: 1.9865, Perplexity: 7.2899\n",
      "Epoch [2/3], Step [2020/4624], Loss: 2.1602, Perplexity: 8.6733\n",
      "Epoch [2/3], Step [2040/4624], Loss: 1.8742, Perplexity: 6.5158\n",
      "Epoch [2/3], Step [2060/4624], Loss: 2.3456, Perplexity: 10.4394\n",
      "Epoch [2/3], Step [2080/4624], Loss: 1.7757, Perplexity: 5.9043\n",
      "Epoch [2/3], Step [2100/4624], Loss: 1.9414, Perplexity: 6.9687\n",
      "Epoch [2/3], Step [2120/4624], Loss: 2.0210, Perplexity: 7.5458\n",
      "Epoch [2/3], Step [2140/4624], Loss: 1.9629, Perplexity: 7.1200\n",
      "Epoch [2/3], Step [2160/4624], Loss: 1.9047, Perplexity: 6.7172\n",
      "Epoch [2/3], Step [2180/4624], Loss: 1.9763, Perplexity: 7.2161\n",
      "Epoch [2/3], Step [2200/4624], Loss: 1.9389, Perplexity: 6.9512\n",
      "Epoch [2/3], Step [2220/4624], Loss: 1.9159, Perplexity: 6.7928\n",
      "Epoch [2/3], Step [2240/4624], Loss: 2.6242, Perplexity: 13.7929\n",
      "Epoch [2/3], Step [2260/4624], Loss: 1.9349, Perplexity: 6.9236\n",
      "Epoch [2/3], Step [2280/4624], Loss: 1.9085, Perplexity: 6.7430\n",
      "Epoch [2/3], Step [2300/4624], Loss: 2.0286, Perplexity: 7.6038\n",
      "Epoch [2/3], Step [2320/4624], Loss: 1.8900, Perplexity: 6.6194\n",
      "Epoch [2/3], Step [2340/4624], Loss: 2.4099, Perplexity: 11.1332\n",
      "Epoch [2/3], Step [2360/4624], Loss: 2.0016, Perplexity: 7.4012\n",
      "Epoch [2/3], Step [2380/4624], Loss: 1.9413, Perplexity: 6.9679\n",
      "Epoch [2/3], Step [2400/4624], Loss: 1.8472, Perplexity: 6.3418\n",
      "Epoch [2/3], Step [2420/4624], Loss: 2.0982, Perplexity: 8.1513\n",
      "Epoch [2/3], Step [2440/4624], Loss: 2.3027, Perplexity: 10.0009\n",
      "Epoch [2/3], Step [2460/4624], Loss: 1.8461, Perplexity: 6.3353\n",
      "Epoch [2/3], Step [2480/4624], Loss: 1.8749, Perplexity: 6.5204\n",
      "Epoch [2/3], Step [2500/4624], Loss: 2.1955, Perplexity: 8.9841\n",
      "Epoch [2/3], Step [2520/4624], Loss: 1.8839, Perplexity: 6.5791\n",
      "Epoch [2/3], Step [2540/4624], Loss: 1.9070, Perplexity: 6.7330\n",
      "Epoch [2/3], Step [2560/4624], Loss: 1.7992, Perplexity: 6.0451\n",
      "Epoch [2/3], Step [2580/4624], Loss: 1.9673, Perplexity: 7.1512\n",
      "Epoch [2/3], Step [2600/4624], Loss: 1.9093, Perplexity: 6.7485\n",
      "Epoch [2/3], Step [2620/4624], Loss: 2.0296, Perplexity: 7.6108\n",
      "Epoch [2/3], Step [2640/4624], Loss: 1.7913, Perplexity: 5.9974\n",
      "Epoch [2/3], Step [2660/4624], Loss: 1.8605, Perplexity: 6.4267\n",
      "Epoch [2/3], Step [2680/4624], Loss: 1.9863, Perplexity: 7.2887\n",
      "Epoch [2/3], Step [2700/4624], Loss: 1.9023, Perplexity: 6.7010\n",
      "Epoch [2/3], Step [2720/4624], Loss: 1.9723, Perplexity: 7.1871\n",
      "Epoch [2/3], Step [2740/4624], Loss: 2.4811, Perplexity: 11.9539\n",
      "Epoch [2/3], Step [2760/4624], Loss: 2.0997, Perplexity: 8.1640\n",
      "Epoch [2/3], Step [2780/4624], Loss: 2.1644, Perplexity: 8.7098\n",
      "Epoch [2/3], Step [2800/4624], Loss: 1.8588, Perplexity: 6.4163\n",
      "Epoch [2/3], Step [2820/4624], Loss: 1.9213, Perplexity: 6.8296\n",
      "Epoch [2/3], Step [2840/4624], Loss: 2.0099, Perplexity: 7.4627\n",
      "Epoch [2/3], Step [2860/4624], Loss: 1.8966, Perplexity: 6.6630\n",
      "Epoch [2/3], Step [2880/4624], Loss: 2.0065, Perplexity: 7.4372\n",
      "Epoch [2/3], Step [2900/4624], Loss: 1.9202, Perplexity: 6.8221\n",
      "Epoch [2/3], Step [2920/4624], Loss: 1.8471, Perplexity: 6.3415\n",
      "Epoch [2/3], Step [2940/4624], Loss: 1.9213, Perplexity: 6.8296\n",
      "Epoch [2/3], Step [2960/4624], Loss: 2.1099, Perplexity: 8.2473\n",
      "Epoch [2/3], Step [2980/4624], Loss: 2.0113, Perplexity: 7.4730\n",
      "Epoch [2/3], Step [3000/4624], Loss: 2.4280, Perplexity: 11.3361\n",
      "Epoch [2/3], Step [3020/4624], Loss: 2.2177, Perplexity: 9.1863\n",
      "Epoch [2/3], Step [3040/4624], Loss: 2.1834, Perplexity: 8.8768\n",
      "Epoch [2/3], Step [3060/4624], Loss: 1.8068, Perplexity: 6.0911\n",
      "Epoch [2/3], Step [3080/4624], Loss: 1.8581, Perplexity: 6.4113\n",
      "Epoch [2/3], Step [3100/4624], Loss: 1.8856, Perplexity: 6.5901\n",
      "Epoch [2/3], Step [3120/4624], Loss: 1.9455, Perplexity: 6.9974\n",
      "Epoch [2/3], Step [3140/4624], Loss: 1.8703, Perplexity: 6.4901\n",
      "Epoch [2/3], Step [3160/4624], Loss: 2.1581, Perplexity: 8.6550\n",
      "Epoch [2/3], Step [3180/4624], Loss: 1.8214, Perplexity: 6.1808\n",
      "Epoch [2/3], Step [3200/4624], Loss: 2.2243, Perplexity: 9.2471\n",
      "Epoch [2/3], Step [3220/4624], Loss: 2.4732, Perplexity: 11.8600\n",
      "Epoch [2/3], Step [3240/4624], Loss: 2.2575, Perplexity: 9.5591\n",
      "Epoch [2/3], Step [3260/4624], Loss: 1.9411, Perplexity: 6.9666\n",
      "Epoch [2/3], Step [3280/4624], Loss: 1.9790, Perplexity: 7.2358\n",
      "Epoch [2/3], Step [3300/4624], Loss: 1.9265, Perplexity: 6.8653\n",
      "Epoch [2/3], Step [3320/4624], Loss: 2.0137, Perplexity: 7.4909\n",
      "Epoch [2/3], Step [3340/4624], Loss: 2.0272, Perplexity: 7.5928\n",
      "Epoch [2/3], Step [3360/4624], Loss: 1.8299, Perplexity: 6.2335\n",
      "Epoch [2/3], Step [3380/4624], Loss: 1.8759, Perplexity: 6.5264\n",
      "Epoch [2/3], Step [3400/4624], Loss: 2.4818, Perplexity: 11.9629\n",
      "Epoch [2/3], Step [3420/4624], Loss: 1.8808, Perplexity: 6.5589\n",
      "Epoch [2/3], Step [3440/4624], Loss: 1.8578, Perplexity: 6.4098\n",
      "Epoch [2/3], Step [3460/4624], Loss: 1.8358, Perplexity: 6.2700\n",
      "Epoch [2/3], Step [3480/4624], Loss: 1.8920, Perplexity: 6.6323\n",
      "Epoch [2/3], Step [3500/4624], Loss: 1.8106, Perplexity: 6.1142\n",
      "Epoch [2/3], Step [3520/4624], Loss: 1.8816, Perplexity: 6.5641\n",
      "Epoch [2/3], Step [3540/4624], Loss: 1.8925, Perplexity: 6.6360\n",
      "Epoch [2/3], Step [3560/4624], Loss: 1.8256, Perplexity: 6.2064\n",
      "Epoch [2/3], Step [3580/4624], Loss: 1.7507, Perplexity: 5.7586\n",
      "Epoch [2/3], Step [3600/4624], Loss: 2.0182, Perplexity: 7.5249\n",
      "Epoch [2/3], Step [3620/4624], Loss: 1.8921, Perplexity: 6.6335\n",
      "Epoch [2/3], Step [3640/4624], Loss: 1.8689, Perplexity: 6.4809\n",
      "Epoch [2/3], Step [3660/4624], Loss: 1.9743, Perplexity: 7.2015\n",
      "Epoch [2/3], Step [3680/4624], Loss: 1.8500, Perplexity: 6.3601\n",
      "Epoch [2/3], Step [3700/4624], Loss: 1.7676, Perplexity: 5.8568\n",
      "Epoch [2/3], Step [3720/4624], Loss: 2.0184, Perplexity: 7.5261\n",
      "Epoch [2/3], Step [3740/4624], Loss: 1.8546, Perplexity: 6.3893\n",
      "Epoch [2/3], Step [3760/4624], Loss: 1.9466, Perplexity: 7.0045\n",
      "Epoch [2/3], Step [3780/4624], Loss: 1.7748, Perplexity: 5.8992\n",
      "Epoch [2/3], Step [3800/4624], Loss: 1.9361, Perplexity: 6.9316\n",
      "Epoch [2/3], Step [3820/4624], Loss: 1.7528, Perplexity: 5.7708\n",
      "Epoch [2/3], Step [3840/4624], Loss: 1.8634, Perplexity: 6.4459\n",
      "Epoch [2/3], Step [3860/4624], Loss: 2.1710, Perplexity: 8.7671\n",
      "Epoch [2/3], Step [3880/4624], Loss: 1.8893, Perplexity: 6.6150\n",
      "Epoch [2/3], Step [3900/4624], Loss: 1.8252, Perplexity: 6.2041\n",
      "Epoch [2/3], Step [3920/4624], Loss: 1.9905, Perplexity: 7.3195\n",
      "Epoch [2/3], Step [3940/4624], Loss: 1.8645, Perplexity: 6.4526\n",
      "Epoch [2/3], Step [3960/4624], Loss: 1.8662, Perplexity: 6.4637\n",
      "Epoch [2/3], Step [3980/4624], Loss: 1.7934, Perplexity: 6.0100\n",
      "Epoch [2/3], Step [4000/4624], Loss: 1.8711, Perplexity: 6.4954\n",
      "Epoch [2/3], Step [4020/4624], Loss: 1.8452, Perplexity: 6.3292\n",
      "Epoch [2/3], Step [4040/4624], Loss: 1.8627, Perplexity: 6.4409\n",
      "Epoch [2/3], Step [4060/4624], Loss: 1.7978, Perplexity: 6.0362\n",
      "Epoch [2/3], Step [4080/4624], Loss: 1.8088, Perplexity: 6.1033\n",
      "Epoch [2/3], Step [4100/4624], Loss: 2.2012, Perplexity: 9.0354\n",
      "Epoch [2/3], Step [4120/4624], Loss: 1.9104, Perplexity: 6.7559\n",
      "Epoch [2/3], Step [4140/4624], Loss: 1.9758, Perplexity: 7.2124\n",
      "Epoch [2/3], Step [4160/4624], Loss: 2.1077, Perplexity: 8.2290\n",
      "Epoch [2/3], Step [4180/4624], Loss: 2.0735, Perplexity: 7.9522\n",
      "Epoch [2/3], Step [4200/4624], Loss: 2.4489, Perplexity: 11.5754\n",
      "Epoch [2/3], Step [4220/4624], Loss: 1.8710, Perplexity: 6.4950\n",
      "Epoch [2/3], Step [4240/4624], Loss: 1.9543, Perplexity: 7.0587\n",
      "Epoch [2/3], Step [4260/4624], Loss: 1.8402, Perplexity: 6.2980\n",
      "Epoch [2/3], Step [4280/4624], Loss: 1.9381, Perplexity: 6.9455\n",
      "Epoch [2/3], Step [4300/4624], Loss: 2.2067, Perplexity: 9.0861\n",
      "Epoch [2/3], Step [4320/4624], Loss: 1.8343, Perplexity: 6.2606\n",
      "Epoch [2/3], Step [4340/4624], Loss: 1.8715, Perplexity: 6.4981\n",
      "Epoch [2/3], Step [4360/4624], Loss: 2.0286, Perplexity: 7.6033\n",
      "Epoch [2/3], Step [4380/4624], Loss: 1.7109, Perplexity: 5.5339\n",
      "Epoch [2/3], Step [4400/4624], Loss: 1.9407, Perplexity: 6.9633\n",
      "Epoch [2/3], Step [4420/4624], Loss: 1.8553, Perplexity: 6.3936\n",
      "Epoch [2/3], Step [4440/4624], Loss: 1.8973, Perplexity: 6.6677\n",
      "Epoch [2/3], Step [4460/4624], Loss: 1.7597, Perplexity: 5.8106\n",
      "Epoch [2/3], Step [4480/4624], Loss: 2.1479, Perplexity: 8.5673\n",
      "Epoch [2/3], Step [4500/4624], Loss: 1.8575, Perplexity: 6.4074\n",
      "Epoch [2/3], Step [4520/4624], Loss: 1.9108, Perplexity: 6.7587\n",
      "Epoch [2/3], Step [4540/4624], Loss: 1.8020, Perplexity: 6.0615\n",
      "Epoch [2/3], Step [4560/4624], Loss: 3.3489, Perplexity: 28.4715\n",
      "Epoch [2/3], Step [4580/4624], Loss: 1.7720, Perplexity: 5.8828\n",
      "Epoch [2/3], Step [4600/4624], Loss: 1.8706, Perplexity: 6.4924\n",
      "Epoch [2/3], Step [4620/4624], Loss: 1.8100, Perplexity: 6.1103\n",
      "Epoch [3/3], Step [20/4624], Loss: 1.8984, Perplexity: 6.6755\n",
      "Epoch [3/3], Step [40/4624], Loss: 2.0366, Perplexity: 7.6641\n",
      "Epoch [3/3], Step [60/4624], Loss: 1.8935, Perplexity: 6.6427\n",
      "Epoch [3/3], Step [80/4624], Loss: 1.8147, Perplexity: 6.1394\n",
      "Epoch [3/3], Step [100/4624], Loss: 1.8317, Perplexity: 6.2447\n",
      "Epoch [3/3], Step [120/4624], Loss: 1.8950, Perplexity: 6.6527\n",
      "Epoch [3/3], Step [140/4624], Loss: 2.0706, Perplexity: 7.9293\n",
      "Epoch [3/3], Step [160/4624], Loss: 1.8840, Perplexity: 6.5796\n",
      "Epoch [3/3], Step [180/4624], Loss: 1.9595, Perplexity: 7.0961\n",
      "Epoch [3/3], Step [200/4624], Loss: 1.9573, Perplexity: 7.0804\n",
      "Epoch [3/3], Step [220/4624], Loss: 1.9116, Perplexity: 6.7639\n",
      "Epoch [3/3], Step [240/4624], Loss: 2.0712, Perplexity: 7.9343\n",
      "Epoch [3/3], Step [260/4624], Loss: 1.8512, Perplexity: 6.3673\n",
      "Epoch [3/3], Step [280/4624], Loss: 1.9191, Perplexity: 6.8147\n",
      "Epoch [3/3], Step [300/4624], Loss: 2.0040, Perplexity: 7.4189\n",
      "Epoch [3/3], Step [320/4624], Loss: 1.8246, Perplexity: 6.2000\n",
      "Epoch [3/3], Step [340/4624], Loss: 2.0427, Perplexity: 7.7118\n",
      "Epoch [3/3], Step [360/4624], Loss: 1.8372, Perplexity: 6.2792\n",
      "Epoch [3/3], Step [380/4624], Loss: 1.6923, Perplexity: 5.4318\n",
      "Epoch [3/3], Step [400/4624], Loss: 1.9392, Perplexity: 6.9529\n",
      "Epoch [3/3], Step [420/4624], Loss: 1.9353, Perplexity: 6.9264\n",
      "Epoch [3/3], Step [440/4624], Loss: 2.4014, Perplexity: 11.0387\n",
      "Epoch [3/3], Step [460/4624], Loss: 1.8193, Perplexity: 6.1674\n",
      "Epoch [3/3], Step [480/4624], Loss: 1.8250, Perplexity: 6.2026\n",
      "Epoch [3/3], Step [500/4624], Loss: 1.9758, Perplexity: 7.2127\n",
      "Epoch [3/3], Step [520/4624], Loss: 2.0542, Perplexity: 7.8006\n",
      "Epoch [3/3], Step [540/4624], Loss: 1.8334, Perplexity: 6.2550\n",
      "Epoch [3/3], Step [560/4624], Loss: 1.8062, Perplexity: 6.0874\n",
      "Epoch [3/3], Step [580/4624], Loss: 1.8089, Perplexity: 6.1040\n",
      "Epoch [3/3], Step [600/4624], Loss: 1.8295, Perplexity: 6.2309\n",
      "Epoch [3/3], Step [620/4624], Loss: 2.1608, Perplexity: 8.6780\n",
      "Epoch [3/3], Step [640/4624], Loss: 1.8616, Perplexity: 6.4340\n",
      "Epoch [3/3], Step [660/4624], Loss: 2.5332, Perplexity: 12.5931\n",
      "Epoch [3/3], Step [680/4624], Loss: 2.0919, Perplexity: 8.1001\n",
      "Epoch [3/3], Step [700/4624], Loss: 1.7263, Perplexity: 5.6200\n",
      "Epoch [3/3], Step [720/4624], Loss: 1.8689, Perplexity: 6.4811\n",
      "Epoch [3/3], Step [740/4624], Loss: 1.7737, Perplexity: 5.8929\n",
      "Epoch [3/3], Step [760/4624], Loss: 1.6647, Perplexity: 5.2841\n",
      "Epoch [3/3], Step [780/4624], Loss: 1.8831, Perplexity: 6.5737\n",
      "Epoch [3/3], Step [800/4624], Loss: 1.8653, Perplexity: 6.4578\n",
      "Epoch [3/3], Step [820/4624], Loss: 1.9509, Perplexity: 7.0353\n",
      "Epoch [3/3], Step [840/4624], Loss: 1.7572, Perplexity: 5.7964\n",
      "Epoch [3/3], Step [860/4624], Loss: 1.7045, Perplexity: 5.4988\n",
      "Epoch [3/3], Step [880/4624], Loss: 1.9031, Perplexity: 6.7064\n",
      "Epoch [3/3], Step [900/4624], Loss: 1.9428, Perplexity: 6.9785\n",
      "Epoch [3/3], Step [920/4624], Loss: 1.8381, Perplexity: 6.2845\n",
      "Epoch [3/3], Step [940/4624], Loss: 1.9830, Perplexity: 7.2648\n",
      "Epoch [3/3], Step [960/4624], Loss: 2.1160, Perplexity: 8.2978\n",
      "Epoch [3/3], Step [980/4624], Loss: 1.9382, Perplexity: 6.9459\n",
      "Epoch [3/3], Step [1000/4624], Loss: 1.7981, Perplexity: 6.0380\n",
      "Epoch [3/3], Step [1020/4624], Loss: 1.8548, Perplexity: 6.3902\n",
      "Epoch [3/3], Step [1040/4624], Loss: 2.0784, Perplexity: 7.9913\n",
      "Epoch [3/3], Step [1060/4624], Loss: 2.7994, Perplexity: 16.4355\n",
      "Epoch [3/3], Step [1080/4624], Loss: 1.7755, Perplexity: 5.9030\n",
      "Epoch [3/3], Step [1100/4624], Loss: 1.7942, Perplexity: 6.0148\n",
      "Epoch [3/3], Step [1120/4624], Loss: 1.7221, Perplexity: 5.5965\n",
      "Epoch [3/3], Step [1140/4624], Loss: 1.9266, Perplexity: 6.8660\n",
      "Epoch [3/3], Step [1160/4624], Loss: 1.8492, Perplexity: 6.3545\n",
      "Epoch [3/3], Step [1180/4624], Loss: 2.2854, Perplexity: 9.8295\n",
      "Epoch [3/3], Step [1200/4624], Loss: 1.9520, Perplexity: 7.0425\n",
      "Epoch [3/3], Step [1220/4624], Loss: 1.8468, Perplexity: 6.3394\n",
      "Epoch [3/3], Step [1240/4624], Loss: 2.0199, Perplexity: 7.5374\n",
      "Epoch [3/3], Step [1260/4624], Loss: 1.8670, Perplexity: 6.4686\n",
      "Epoch [3/3], Step [1280/4624], Loss: 1.7985, Perplexity: 6.0404\n",
      "Epoch [3/3], Step [1300/4624], Loss: 1.7642, Perplexity: 5.8367\n",
      "Epoch [3/3], Step [1320/4624], Loss: 1.7293, Perplexity: 5.6368\n",
      "Epoch [3/3], Step [1340/4624], Loss: 1.9327, Perplexity: 6.9085\n",
      "Epoch [3/3], Step [1360/4624], Loss: 2.0138, Perplexity: 7.4915\n",
      "Epoch [3/3], Step [1380/4624], Loss: 1.7830, Perplexity: 5.9479\n",
      "Epoch [3/3], Step [1400/4624], Loss: 1.8446, Perplexity: 6.3254\n",
      "Epoch [3/3], Step [1420/4624], Loss: 1.8645, Perplexity: 6.4525\n",
      "Epoch [3/3], Step [1440/4624], Loss: 2.0139, Perplexity: 7.4928\n",
      "Epoch [3/3], Step [1460/4624], Loss: 1.9228, Perplexity: 6.8401\n",
      "Epoch [3/3], Step [1480/4624], Loss: 1.8232, Perplexity: 6.1914\n",
      "Epoch [3/3], Step [1500/4624], Loss: 2.3101, Perplexity: 10.0756\n",
      "Epoch [3/3], Step [1520/4624], Loss: 1.9408, Perplexity: 6.9644\n",
      "Epoch [3/3], Step [1540/4624], Loss: 2.1672, Perplexity: 8.7342\n",
      "Epoch [3/3], Step [1560/4624], Loss: 1.8660, Perplexity: 6.4623\n",
      "Epoch [3/3], Step [1580/4624], Loss: 2.0339, Perplexity: 7.6438\n",
      "Epoch [3/3], Step [1600/4624], Loss: 2.3495, Perplexity: 10.4800\n",
      "Epoch [3/3], Step [1620/4624], Loss: 2.1211, Perplexity: 8.3401\n",
      "Epoch [3/3], Step [1640/4624], Loss: 1.9281, Perplexity: 6.8762\n",
      "Epoch [3/3], Step [1660/4624], Loss: 1.8577, Perplexity: 6.4088\n",
      "Epoch [3/3], Step [1680/4624], Loss: 1.8116, Perplexity: 6.1203\n",
      "Epoch [3/3], Step [1700/4624], Loss: 1.7157, Perplexity: 5.5603\n",
      "Epoch [3/3], Step [1720/4624], Loss: 1.7922, Perplexity: 6.0026\n",
      "Epoch [3/3], Step [1740/4624], Loss: 1.9769, Perplexity: 7.2205\n",
      "Epoch [3/3], Step [1760/4624], Loss: 1.6839, Perplexity: 5.3863\n",
      "Epoch [3/3], Step [1780/4624], Loss: 1.7888, Perplexity: 5.9823\n",
      "Epoch [3/3], Step [1800/4624], Loss: 1.9651, Perplexity: 7.1353\n",
      "Epoch [3/3], Step [1820/4624], Loss: 1.7860, Perplexity: 5.9653\n",
      "Epoch [3/3], Step [1840/4624], Loss: 1.8916, Perplexity: 6.6302\n",
      "Epoch [3/3], Step [1860/4624], Loss: 1.9817, Perplexity: 7.2554\n",
      "Epoch [3/3], Step [1880/4624], Loss: 2.0674, Perplexity: 7.9039\n",
      "Epoch [3/3], Step [1900/4624], Loss: 1.8074, Perplexity: 6.0944\n",
      "Epoch [3/3], Step [1920/4624], Loss: 1.6835, Perplexity: 5.3841\n",
      "Epoch [3/3], Step [1940/4624], Loss: 1.8593, Perplexity: 6.4194\n",
      "Epoch [3/3], Step [1960/4624], Loss: 1.6881, Perplexity: 5.4092\n",
      "Epoch [3/3], Step [1980/4624], Loss: 2.7181, Perplexity: 15.1511\n",
      "Epoch [3/3], Step [2000/4624], Loss: 1.8360, Perplexity: 6.2717\n",
      "Epoch [3/3], Step [2020/4624], Loss: 1.8084, Perplexity: 6.1007\n",
      "Epoch [3/3], Step [2040/4624], Loss: 2.0108, Perplexity: 7.4692\n",
      "Epoch [3/3], Step [2060/4624], Loss: 1.7573, Perplexity: 5.7970\n",
      "Epoch [3/3], Step [2080/4624], Loss: 1.8764, Perplexity: 6.5302\n",
      "Epoch [3/3], Step [2100/4624], Loss: 1.8374, Perplexity: 6.2801\n",
      "Epoch [3/3], Step [2120/4624], Loss: 1.7526, Perplexity: 5.7696\n",
      "Epoch [3/3], Step [2140/4624], Loss: 1.8827, Perplexity: 6.5713\n",
      "Epoch [3/3], Step [2160/4624], Loss: 1.8190, Perplexity: 6.1656\n",
      "Epoch [3/3], Step [2180/4624], Loss: 1.6478, Perplexity: 5.1954\n",
      "Epoch [3/3], Step [2200/4624], Loss: 1.9064, Perplexity: 6.7288\n",
      "Epoch [3/3], Step [2220/4624], Loss: 1.9627, Perplexity: 7.1185\n",
      "Epoch [3/3], Step [2240/4624], Loss: 1.8775, Perplexity: 6.5374\n",
      "Epoch [3/3], Step [2260/4624], Loss: 1.8375, Perplexity: 6.2809\n",
      "Epoch [3/3], Step [2280/4624], Loss: 1.8332, Perplexity: 6.2537\n",
      "Epoch [3/3], Step [2300/4624], Loss: 2.0382, Perplexity: 7.6769\n",
      "Epoch [3/3], Step [2320/4624], Loss: 2.1065, Perplexity: 8.2198\n",
      "Epoch [3/3], Step [2340/4624], Loss: 1.9212, Perplexity: 6.8293\n",
      "Epoch [3/3], Step [2360/4624], Loss: 2.1783, Perplexity: 8.8311\n",
      "Epoch [3/3], Step [2380/4624], Loss: 2.2793, Perplexity: 9.7701\n",
      "Epoch [3/3], Step [2400/4624], Loss: 1.7783, Perplexity: 5.9200\n",
      "Epoch [3/3], Step [2420/4624], Loss: 1.9514, Perplexity: 7.0386\n",
      "Epoch [3/3], Step [2440/4624], Loss: 2.0396, Perplexity: 7.6872\n",
      "Epoch [3/3], Step [2460/4624], Loss: 2.3751, Perplexity: 10.7522\n",
      "Epoch [3/3], Step [2480/4624], Loss: 2.1399, Perplexity: 8.4990\n",
      "Epoch [3/3], Step [2500/4624], Loss: 1.9412, Perplexity: 6.9674\n",
      "Epoch [3/3], Step [2520/4624], Loss: 1.9127, Perplexity: 6.7713\n",
      "Epoch [3/3], Step [2540/4624], Loss: 1.8692, Perplexity: 6.4829\n",
      "Epoch [3/3], Step [2560/4624], Loss: 1.8408, Perplexity: 6.3018\n",
      "Epoch [3/3], Step [2580/4624], Loss: 1.7243, Perplexity: 5.6085\n",
      "Epoch [3/3], Step [2600/4624], Loss: 2.5482, Perplexity: 12.7844\n",
      "Epoch [3/3], Step [2620/4624], Loss: 1.7729, Perplexity: 5.8877\n",
      "Epoch [3/3], Step [2640/4624], Loss: 1.6638, Perplexity: 5.2795\n",
      "Epoch [3/3], Step [2660/4624], Loss: 1.8882, Perplexity: 6.6077\n",
      "Epoch [3/3], Step [2680/4624], Loss: 1.7781, Perplexity: 5.9185\n",
      "Epoch [3/3], Step [2700/4624], Loss: 1.7468, Perplexity: 5.7362\n",
      "Epoch [3/3], Step [2720/4624], Loss: 1.9117, Perplexity: 6.7645\n",
      "Epoch [3/3], Step [2740/4624], Loss: 1.8377, Perplexity: 6.2823\n",
      "Epoch [3/3], Step [2760/4624], Loss: 1.7634, Perplexity: 5.8324\n",
      "Epoch [3/3], Step [2780/4624], Loss: 1.9584, Perplexity: 7.0879\n",
      "Epoch [3/3], Step [2800/4624], Loss: 1.7036, Perplexity: 5.4935\n",
      "Epoch [3/3], Step [2820/4624], Loss: 1.8425, Perplexity: 6.3126\n",
      "Epoch [3/3], Step [2840/4624], Loss: 2.2481, Perplexity: 9.4697\n",
      "Epoch [3/3], Step [2860/4624], Loss: 1.9066, Perplexity: 6.7299\n",
      "Epoch [3/3], Step [2880/4624], Loss: 1.7753, Perplexity: 5.9020\n",
      "Epoch [3/3], Step [2900/4624], Loss: 1.7804, Perplexity: 5.9321\n",
      "Epoch [3/3], Step [2920/4624], Loss: 1.7490, Perplexity: 5.7491\n",
      "Epoch [3/3], Step [2940/4624], Loss: 1.9502, Perplexity: 7.0304\n",
      "Epoch [3/3], Step [2960/4624], Loss: 1.7803, Perplexity: 5.9319\n",
      "Epoch [3/3], Step [2980/4624], Loss: 1.6837, Perplexity: 5.3857\n",
      "Epoch [3/3], Step [3000/4624], Loss: 1.9410, Perplexity: 6.9658\n",
      "Epoch [3/3], Step [3020/4624], Loss: 1.9838, Perplexity: 7.2705\n",
      "Epoch [3/3], Step [3040/4624], Loss: 1.9266, Perplexity: 6.8659\n",
      "Epoch [3/3], Step [3060/4624], Loss: 1.8023, Perplexity: 6.0635\n",
      "Epoch [3/3], Step [3080/4624], Loss: 2.0874, Perplexity: 8.0636\n",
      "Epoch [3/3], Step [3100/4624], Loss: 1.7347, Perplexity: 5.6674\n",
      "Epoch [3/3], Step [3120/4624], Loss: 1.7108, Perplexity: 5.5333\n",
      "Epoch [3/3], Step [3140/4624], Loss: 2.0728, Perplexity: 7.9473\n",
      "Epoch [3/3], Step [3160/4624], Loss: 1.8172, Perplexity: 6.1549\n",
      "Epoch [3/3], Step [3180/4624], Loss: 2.0471, Perplexity: 7.7455\n",
      "Epoch [3/3], Step [3200/4624], Loss: 1.8620, Perplexity: 6.4367\n",
      "Epoch [3/3], Step [3220/4624], Loss: 1.9807, Perplexity: 7.2476\n",
      "Epoch [3/3], Step [3240/4624], Loss: 1.8332, Perplexity: 6.2537\n",
      "Epoch [3/3], Step [3260/4624], Loss: 1.7872, Perplexity: 5.9727\n",
      "Epoch [3/3], Step [3280/4624], Loss: 1.7233, Perplexity: 5.6031\n",
      "Epoch [3/3], Step [3300/4624], Loss: 1.8966, Perplexity: 6.6635\n",
      "Epoch [3/3], Step [3320/4624], Loss: 1.7629, Perplexity: 5.8296\n",
      "Epoch [3/3], Step [3340/4624], Loss: 1.7784, Perplexity: 5.9206\n",
      "Epoch [3/3], Step [3360/4624], Loss: 1.8825, Perplexity: 6.5697\n",
      "Epoch [3/3], Step [3380/4624], Loss: 1.7039, Perplexity: 5.4954\n",
      "Epoch [3/3], Step [3400/4624], Loss: 1.6783, Perplexity: 5.3564\n",
      "Epoch [3/3], Step [3420/4624], Loss: 1.9087, Perplexity: 6.7440\n",
      "Epoch [3/3], Step [3440/4624], Loss: 1.8571, Perplexity: 6.4054\n",
      "Epoch [3/3], Step [3460/4624], Loss: 1.8650, Perplexity: 6.4556\n",
      "Epoch [3/3], Step [3480/4624], Loss: 2.0326, Perplexity: 7.6341\n",
      "Epoch [3/3], Step [3500/4624], Loss: 1.7938, Perplexity: 6.0123\n",
      "Epoch [3/3], Step [3520/4624], Loss: 1.7860, Perplexity: 5.9657\n",
      "Epoch [3/3], Step [3540/4624], Loss: 1.7693, Perplexity: 5.8670\n",
      "Epoch [3/3], Step [3560/4624], Loss: 1.9282, Perplexity: 6.8768\n",
      "Epoch [3/3], Step [3580/4624], Loss: 2.0184, Perplexity: 7.5264\n",
      "Epoch [3/3], Step [3600/4624], Loss: 2.0711, Perplexity: 7.9337\n",
      "Epoch [3/3], Step [3620/4624], Loss: 1.8562, Perplexity: 6.3994\n",
      "Epoch [3/3], Step [3640/4624], Loss: 1.8517, Perplexity: 6.3710\n",
      "Epoch [3/3], Step [3660/4624], Loss: 1.8057, Perplexity: 6.0841\n",
      "Epoch [3/3], Step [3680/4624], Loss: 1.8796, Perplexity: 6.5508\n",
      "Epoch [3/3], Step [3700/4624], Loss: 1.8388, Perplexity: 6.2891\n",
      "Epoch [3/3], Step [3720/4624], Loss: 1.6243, Perplexity: 5.0747\n",
      "Epoch [3/3], Step [3740/4624], Loss: 1.8714, Perplexity: 6.4972\n",
      "Epoch [3/3], Step [3760/4624], Loss: 1.8030, Perplexity: 6.0677\n",
      "Epoch [3/3], Step [3780/4624], Loss: 2.1832, Perplexity: 8.8750\n",
      "Epoch [3/3], Step [3800/4624], Loss: 1.6969, Perplexity: 5.4572\n",
      "Epoch [3/3], Step [3820/4624], Loss: 1.8144, Perplexity: 6.1376\n",
      "Epoch [3/3], Step [3840/4624], Loss: 1.8728, Perplexity: 6.5064\n",
      "Epoch [3/3], Step [3860/4624], Loss: 1.7885, Perplexity: 5.9807\n",
      "Epoch [3/3], Step [3880/4624], Loss: 1.9454, Perplexity: 6.9961\n",
      "Epoch [3/3], Step [3900/4624], Loss: 1.7543, Perplexity: 5.7792\n",
      "Epoch [3/3], Step [3920/4624], Loss: 2.4605, Perplexity: 11.7107\n",
      "Epoch [3/3], Step [3940/4624], Loss: 1.6966, Perplexity: 5.4555\n",
      "Epoch [3/3], Step [3960/4624], Loss: 1.7957, Perplexity: 6.0239\n",
      "Epoch [3/3], Step [3980/4624], Loss: 2.6119, Perplexity: 13.6255\n",
      "Epoch [3/3], Step [4000/4624], Loss: 1.7802, Perplexity: 5.9311\n",
      "Epoch [3/3], Step [4020/4624], Loss: 1.8259, Perplexity: 6.2084\n",
      "Epoch [3/3], Step [4040/4624], Loss: 1.7384, Perplexity: 5.6884\n",
      "Epoch [3/3], Step [4060/4624], Loss: 1.8577, Perplexity: 6.4089\n",
      "Epoch [3/3], Step [4080/4624], Loss: 2.0386, Perplexity: 7.6799\n",
      "Epoch [3/3], Step [4100/4624], Loss: 1.7254, Perplexity: 5.6150\n",
      "Epoch [3/3], Step [4120/4624], Loss: 1.8615, Perplexity: 6.4335\n",
      "Epoch [3/3], Step [4140/4624], Loss: 1.9869, Perplexity: 7.2926\n",
      "Epoch [3/3], Step [4160/4624], Loss: 1.7855, Perplexity: 5.9626\n",
      "Epoch [3/3], Step [4180/4624], Loss: 2.2021, Perplexity: 9.0439\n",
      "Epoch [3/3], Step [4200/4624], Loss: 1.6851, Perplexity: 5.3932\n",
      "Epoch [3/3], Step [4220/4624], Loss: 1.7757, Perplexity: 5.9041\n",
      "Epoch [3/3], Step [4240/4624], Loss: 1.7747, Perplexity: 5.8985\n",
      "Epoch [3/3], Step [4260/4624], Loss: 1.8973, Perplexity: 6.6680\n",
      "Epoch [3/3], Step [4280/4624], Loss: 1.7481, Perplexity: 5.7434\n",
      "Epoch [3/3], Step [4300/4624], Loss: 1.6801, Perplexity: 5.3662\n",
      "Epoch [3/3], Step [4320/4624], Loss: 2.1141, Perplexity: 8.2823\n",
      "Epoch [3/3], Step [4340/4624], Loss: 2.0219, Perplexity: 7.5524\n",
      "Epoch [3/3], Step [4360/4624], Loss: 1.8632, Perplexity: 6.4446\n",
      "Epoch [3/3], Step [4380/4624], Loss: 1.7940, Perplexity: 6.0132\n",
      "Epoch [3/3], Step [4400/4624], Loss: 1.8702, Perplexity: 6.4895\n",
      "Epoch [3/3], Step [4420/4624], Loss: 1.8422, Perplexity: 6.3102\n",
      "Epoch [3/3], Step [4440/4624], Loss: 1.8357, Perplexity: 6.2695\n",
      "Epoch [3/3], Step [4460/4624], Loss: 1.8482, Perplexity: 6.3482\n",
      "Epoch [3/3], Step [4480/4624], Loss: 1.7390, Perplexity: 5.6918\n",
      "Epoch [3/3], Step [4500/4624], Loss: 1.8381, Perplexity: 6.2846\n",
      "Epoch [3/3], Step [4520/4624], Loss: 1.9041, Perplexity: 6.7137\n",
      "Epoch [3/3], Step [4540/4624], Loss: 1.7854, Perplexity: 5.9621\n",
      "Epoch [3/3], Step [4560/4624], Loss: 2.4048, Perplexity: 11.0761\n",
      "Epoch [3/3], Step [4580/4624], Loss: 1.7271, Perplexity: 5.6243\n",
      "Epoch [3/3], Step [4600/4624], Loss: 1.9116, Perplexity: 6.7637\n",
      "Epoch [3/3], Step [4620/4624], Loss: 1.7465, Perplexity: 5.7347\n"
     ]
    }
   ],
   "source": [
    "# Open the training log file.\n",
    "f = open(log_file, \"w\")\n",
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for i_step in range(1, total_step + 1):\n",
    "\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # Passing the inputs through the CNN-RNN model\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculating the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "        # Backwarding pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Getting training statistics\n",
    "        stats = (\n",
    "            f\"Epoch [{epoch}/{num_epochs}], Step [{i_step}/{total_step}], \"\n",
    "            f\"Loss: {loss.item():.4f}, Perplexity: {np.exp(loss.item()):.4f}\"\n",
    "        )\n",
    "\n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print(\"\\r\" + stats)\n",
    "\n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(\n",
    "            decoder.state_dict(), os.path.join(\"./models\", \"decoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "        torch.save(\n",
    "            encoder.state_dict(), os.path.join(\"./models\", \"encoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Outputs in Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T13:05:36.632321Z",
     "iopub.status.busy": "2025-02-21T13:05:36.631983Z",
     "iopub.status.idle": "2025-02-21T13:06:13.573787Z",
     "shell.execute_reply": "2025-02-21T13:06:13.572671Z",
     "shell.execute_reply.started": "2025-02-21T13:05:36.632288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/ImageCapionCoCoResults_models.zip'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"ImageCapionCoCoResults_models\", 'zip', \"/kaggle/working/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T13:06:13.575579Z",
     "iopub.status.busy": "2025-02-21T13:06:13.575294Z",
     "iopub.status.idle": "2025-02-21T13:06:13.649121Z",
     "shell.execute_reply": "2025-02-21T13:06:13.647915Z",
     "shell.execute_reply.started": "2025-02-21T13:06:13.575550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/ImageCapionCoCoResults_data.zip'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"ImageCapionCoCoResults_data\", 'zip', \"/kaggle/working/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T13:06:13.652123Z",
     "iopub.status.busy": "2025-02-21T13:06:13.651748Z",
     "iopub.status.idle": "2025-02-21T13:06:14.886118Z",
     "shell.execute_reply": "2025-02-21T13:06:14.885265Z",
     "shell.execute_reply.started": "2025-02-21T13:06:13.652090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.vocab_file :  /kaggle/working/data/vocab.pkl\n",
      "file : <_io.BufferedReader name='/kaggle/working/data/vocab.pkl'>\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/3408490603.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(os.path.join(\"./models\", encoder_file)))\n",
      "/tmp/ipykernel_23/3408490603.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load(os.path.join(\"./models\", decoder_file)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(16807, 512)\n",
       "  (lstm): LSTM(512, 1024, batch_first=True)\n",
       "  (linear): Linear(in_features=1024, out_features=16807, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "#Create the data loader.\n",
    "val_data_loader = get_loader(\n",
    "    transform=transform_test, mode=\"valid\", cocoapi_loc=coco_dataset_path\n",
    ")\n",
    "\n",
    "\n",
    "encoder_file = \"encoder-3.pkl\"\n",
    "decoder_file = \"decoder-3.pkl\"\n",
    "\n",
    "# Initialize the encoder and decoder.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Moving models to GPU if CUDA is available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Loading the trained weights\n",
    "encoder.load_state_dict(torch.load(os.path.join(\"./models\", encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join(\"./models\", decoder_file)))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T13:06:14.887499Z",
     "iopub.status.busy": "2025-02-21T13:06:14.887229Z",
     "iopub.status.idle": "2025-02-21T13:06:14.926491Z",
     "shell.execute_reply": "2025-02-21T13:06:14.925733Z",
     "shell.execute_reply.started": "2025-02-21T13:06:14.887474Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "def clean_sentence(output, idx2word):\n",
    "    sentence = \"\"\n",
    "    for i in output:\n",
    "        word = idx2word[i]\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == 1:\n",
    "            break\n",
    "        if i == 18:\n",
    "            sentence = sentence + word\n",
    "        else:\n",
    "            sentence = sentence + \" \" + word\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def bleu_score(true_sentences, predicted_sentences):\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    for img_id in set(true_sentences.keys()).intersection(\n",
    "        set(predicted_sentences.keys())\n",
    "    ):\n",
    "        img_refs = [cap.split() for cap in true_sentences[img_id]]\n",
    "        references.append(img_refs)\n",
    "        hypotheses.append(predicted_sentences[img_id][0].strip().split())\n",
    "\n",
    "    return corpus_bleu(references, hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T13:06:14.928440Z",
     "iopub.status.busy": "2025-02-21T13:06:14.927790Z",
     "iopub.status.idle": "2025-02-21T13:09:24.182848Z",
     "shell.execute_reply": "2025-02-21T13:09:24.181897Z",
     "shell.execute_reply.started": "2025-02-21T13:06:14.928397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526ea2d74b354b15bddcdc02f03f4a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# infer captions for all images\n",
    "pred_result = defaultdict(list)\n",
    "for img_id, img in tqdm(val_data_loader):\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = encoder(img).unsqueeze(1)\n",
    "        output = decoder.sample(features)\n",
    "    sentence = clean_sentence(output, val_data_loader.dataset.vocab.idx2word)\n",
    "    pred_result[img_id.item()].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T13:09:24.184886Z",
     "iopub.status.busy": "2025-02-21T13:09:24.184290Z",
     "iopub.status.idle": "2025-02-21T13:09:24.274616Z",
     "shell.execute_reply": "2025-02-21T13:09:24.273864Z",
     "shell.execute_reply.started": "2025-02-21T13:09:24.184844Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(coco_dataset_path, coco2017_val_annotation_path), \"r\"\n",
    ") as f:\n",
    "    caption = json.load(f)\n",
    "\n",
    "valid_annot = caption[\"annotations\"]\n",
    "valid_result = defaultdict(list)\n",
    "for i in valid_annot:\n",
    "    valid_result[i[\"image_id\"]].append(i[\"caption\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T13:09:24.276463Z",
     "iopub.status.busy": "2025-02-21T13:09:24.275777Z",
     "iopub.status.idle": "2025-02-21T13:09:24.313083Z",
     "shell.execute_reply": "2025-02-21T13:09:24.312195Z",
     "shell.execute_reply.started": "2025-02-21T13:09:24.276423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a black honda motorcycle parked in front of a garage.',\n",
       "  'a honda motorcycle parked in a grass driveway',\n",
       "  'a black honda motorcycle with a dark burgundy seat.',\n",
       "  'ma motorcycle parked on the gravel in front of a garage',\n",
       "  'a motorcycle with its brake extended standing outside'],\n",
       " ['an office cubicle with four different types of computers.',\n",
       "  'the home office space seems to be very cluttered.',\n",
       "  'an office with desk computer and chair and laptop.',\n",
       "  'office setting with a lot of computer screens.',\n",
       "  'a desk and chair in an office cubicle.'],\n",
       " ['a small closed toilet in a cramped space.',\n",
       "  'a tan toilet and sink combination in a small room.',\n",
       "  'this is an advanced toilet with a sink and control panel.',\n",
       "  'a close-up picture of a toilet with a fountain.',\n",
       "  'off white toilet with a faucet and controls. ']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(valid_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T13:09:24.314624Z",
     "iopub.status.busy": "2025-02-21T13:09:24.314279Z",
     "iopub.status.idle": "2025-02-21T13:09:24.351815Z",
     "shell.execute_reply": "2025-02-21T13:09:24.351011Z",
     "shell.execute_reply.started": "2025-02-21T13:09:24.314585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' a person is riding a motorcycle on a road .'],\n",
       " [' a man is sitting on a bench with a dog .'],\n",
       " [' a person is holding a video game controller']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pred_result.values())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bleu Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T13:09:24.354828Z",
     "iopub.status.busy": "2025-02-21T13:09:24.354254Z",
     "iopub.status.idle": "2025-02-21T13:09:25.403925Z",
     "shell.execute_reply": "2025-02-21T13:09:25.403014Z",
     "shell.execute_reply.started": "2025-02-21T13:09:24.354795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22607629895760276"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score(true_sentences=valid_result, predicted_sentences=pred_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
